%!TEX root = ../thesis_a4.tex

\chapter{Data Pre-processing}
\label{chap:data_preprocessing}


\section{Introduction}
\label{sec:data_preprocessing_intro}

In this chapter we describe the pre-processing steps followed to obtain different data representations from audio music signals. These compact and musically meaningful representations derived from raw audio data are used as input to different methods described later in this dissertation. Since a number these methods start by performing the same data transformation, we consolidate and present these common pre-processing blocks in the same chapter. This chapter is based on work already presented in XXXX. In the subsequent sections we describe the process followed for identifying the tonic of the lead artist XXXX, for estimating the pitch of the predominant melodic source (typically the lead artist) and transforming it from Hertz to Cents scale, and detecting \gls{nyas} segments in the melodies of \gls{iam}.

Along with the description of these processing blocks we also provide sufficient implementation details and the parameter values used in the experiments. For certain experiments the parameter settings might differ from the one presented in this chapter. In such cases the precise value of the parameters is given in the description of the respective method.

\TODO{Do we need to provide the motivation or need for data pre-processing or this is obvious?}

%In this thesis we follow a data-driven methodology to devise computational models for melodic analysis of \gls{iam}.  In the scope of this work data primarily corresponds to a collection of annotated audio music recordings. Before we start to model the human perception and cognition functions associated with a task, we transform the raw audio data and derive a higher level representation. Such an abstracted representation compactly captures the essential musical characteristics that XXXX. 
%
%In this chapter we present different representations of the data used across the experiments in this thesis and describe the procedure to extract them from the raw audio signals. Since several experiments share the step of data transformation, it is summarized in this chapter at once. 


%Thoughts
%\begin{itemize}
%	\item What do we mean by data here, what kind of data are we using. Data-> input that we feed to our algorithms. In the scope of this thesis its primarily the audio data + small amount of metadata. The pre-processing is only for the audio data. 
%	\item What do we mean by data pre-proessing. why is it called pre-processing. Audio data is a very low level physical data, before we can use it or make sense out of it we have to abstract it. We have to transform the data and extract a representation of the concept that we are working on. This is how most of the algorithms discussed in this paper function. Typically these processing involve converting audio data to a perceptual representation. On which further cognitive models are applied to extract high level information which has a musical interpretation or is musically meaningful. 
%	\item Why do we have to do pre-processing, what is the reason/motivation
%	\item What kind of pre-processing do we do. 
%	\item Why have we written this at the beginning at one place, its been used in almost every module in the paper
%	\item How is this chapter organized. 
%\end{itemize}

\section{Tonic Identification: A Comparative Evaluation}
\label{sec:data_preprocessing_tonic_identification}

The tonic pitch of a lead artist is the base frequency that serves as a reference and foundation for melodic integration throughout the performance (Section\TODO{secref}). All the tones in the musical progression are always in reference and related to the tonic pitch. The accompanying instruments such as tabl\={a} and violin are tuned to the tonic of the lead performer. In a performance of \gls{iam} tonic pitch in majority of the cases corresponds to the \textit{Sa} \gls{svara} of the \gls{raga} (the exceptions are the madhyam \gls{shruti} cases in Carnatic music). Other \glspl{svara} used in the performance derive their meaning and purpose in relation to the \textit{Sa} \gls{svara} and the tonal context established by the particular \gls{raga}~\citep{Danielou2010}. Identification of tonic pitch is therefore a crucial first step for tonal analysis of \gls{iam}. In Particular, for a meaningful comparison of melodies across artists, it is important that the melodic representation is normalized by the tonic of the lead artist. Identification of the tonic pitch of the lead artist in a recording is therefore typically a pre-processing step in most of the melodic analyses of \gls{iam}. 

There exist a number of approaches for identifying tonic pitch in an audio recording of \gls{iam}~\citep{salamon2012multipitch,gulati2012two,bellur2012knowledge,ranjani2011carnatic,Sengupta2005b}\TODO{{add parags method and then say why it wasn't considered for the evaluation}}. Our previous works on tonic identification show promising results with accuracy close to 90\%~\citep{salamon2012multipitch,gulati2012two}. However, it might be misleading to draw a consensus on the best performing method based on just the reported accuracy. This is because these approaches are evaluated using different datasets with varied musical characteristics and under different experimental setup. Therefore, to select the most promising approach we perform a thorough comparative evaluation on the same set of datasets and under the same experimental setup~\citep{Gulati2014Tonic}. The evaluation is performed using a number of scalable datasets with varied musical and acoustical characteristics that are representative of real-world collections of \gls{iam}. In the subsequent sections we describe the comparative evaluation, which is based on our earlier work presented in~\cite{Gulati2014Tonic}. In order for this dissertation to be self-contained, we start by providing a brief description of the methods we evaluated~(\secref{sec:pre_processing_tonic_identification_methods}). We then describe the experimental setup and the datasets used for the comparative evaluation~(\secref{sec:pre_processing_experimental_setup}). We present the results of the comparative evaluation and highlight the strengths and limitations of the methods on different type of music material~(\secref{sec:pre_processing_tonic_identification_results}). Finally, we explain a heuristic-based approach to correct common mistakes in tonic identification in a collection of \gls{iam} by exploiting associated metadata of a recording. 


\subsection{Tonic Identification Methods}
\label{sec:pre_processing_tonic_identification_methods}

As mentioned, there have been various efforts to automatically identify the tonic pitch of the
lead artist in a performance of Indian art music~\citep{salamon2012multipitch,gulati2012two,bellur2012knowledge,ranjani2011carnatic,Sengupta2005b}.
These approaches mainly differ in terms of the musical cues that they utilize to
identify the tonic, the amount of input audio data used to perform this task and the type
of music material they are devised for (Hindustani or Carnatic, vocal or
instrumental, etc.). Despite the differences, all these approaches can be divided
into three main processing blocks, as shown in~\figref{fig:tonic_identification_general_block_diagram}. The only exception to this schema is the approach proposed by~\citep{Sengupta2005b}.

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeNinety]{ch05_preprocessing/figures/tonic_identification_block_diagram.pdf}
	\end{center}
	\caption[General block diagram of the processing steps used by tonic identification
	approaches.]{General block diagram of the processing steps used by tonic identification
		approaches.}
	\label{fig:tonic_identification_general_block_diagram}
\end{figure}


In all the aforementioned approaches, the three main processing blocks are the following: feature
extraction, feature distribution estimation and tonic selection. Since the task
of tonic identification involves an analysis of the tonal content of the audio
signal, the features extracted in the first block are always pitch related. In
the second block, an estimate of the distribution of these features is obtained
using either Parzen window based density estimation or by constructing a
histogram. The feature distribution is then used in the third block to identify
the tonic. The peaks of the distribution correspond to the most salient pitch
values used in the performance (usually the svars of the \gls{raga}), one of which
corresponds to the tonic pitch. As the most salient peak in the distribution is
not guaranteed to be the tonic, various techniques are applied to select the peak that corresponds to the tonic.



{\renewcommand{\arraystretch}{1.5}
	\begin{sidewaystable} 
		\begin{centering}
			\begin{tabular}{ c c c c }
\tabletop			
			Method 	&	Features	&	Feature Distribution	&	Tonic Selection \\
\tablemid			
			\acrshort{tonicid_sengupta} \citep{Sengupta2005b}	&	Pitch \citep{AKDatta_1996} & N/A & Error minimization\\
			
			\acrshort{tonicid_ranjani_1}/\acrshort{tonicid_ranjani_2} \citep{ranjani2011carnatic}	&	Pitch \citep{BoersmaPaul2001} & Parzen-window-based \acrshort{pde}  & GMM fitting\\
			
			\acrshort{tonicid_justin} \citep{salamon2012multipitch} & Multi-pitch salience \citep{Salamon2011} & Multi-pitch histogram & Decision tree\\

			\acrshort{tonicid_sankalp} \citep{gulati2012two}	& Multi-pitch salience  \citep{Salamon2011} & Multi-pitch histogram & Decision tree\\

			&	Predominant melody \citep{Salamon2012} & Pitch histogram & Decision tree\\

			\acrshort{tonicid_ashwin_1} \citep{bellur2012knowledge}	&	Pitch \citep{DeCheveigne2002}	&  \acrshort{gd} histogram & Highest peak\\

			\acrshort{tonicid_ashwin_2} \citep{bellur2012knowledge}	&	Pitch \citep{DeCheveigne2002}	& 	GD histogram	&
			Template matching\\

			\acrshort{tonicid_ashwin_2} \citep{bellur2012knowledge}	&	Pitch \citep{DeCheveigne2002}	& 	GD histogram
			& Highest peak\\
			
			\acrshort{tonicid_chordia}	& 	& 	& \\			

\tablebot			
			\end{tabular}

			\caption[Summary of existing tonic identification approaches.]{Summary of existing tonic identification approaches.}
			\label{tab:pre_processing_tonic_identification_summary_methods}
			\par \end{centering}	
	\end{sidewaystable}


In \tabref{tab:pre_processing_tonic_identification_summary_methods} we provide a summary of the existing methods for tonic identification. The common processing blocks and the main differences between them become evident from this table. A detailed review of these methods in terms of the three processing stages as shown in \figref{fig:tonic_identification_general_block_diagram} is done in~\cite{Gulati2014Tonic}. For a more detailed description of these methods we refer to their respective publications listed in Table \tabref{tab:pre_processing_tonic_identification_summary_methods}. 

\TODO{description of the methods?}

\subsection{Comparative Evaluation Setup}
\label{sec:pre_processing_experimental_setup}

In order to select the best method for tonic identification in audio collections of \gls{iam} we perform a comparative evaluation. We evaluate seven of the nine tonic identification methods listed in \tabref{tab:pre_processing_tonic_identification_summary_methods} using the same set of datasets. The methods chosen for evaluation are
denoted by \acrshort{tonicid_justin}~\citep{salamon2012multipitch}, \acrshort{tonicid_sankalp}~\citep{gulati2012two}, \acrshort{tonicid_ranjani_1} and \acrshort{tonicid_ranjani_2}~\citep{ranjani2011carnatic}, \acrshort{tonicid_ashwin_1}, \acrshort{tonicid_ashwin_2} and \acrshort{tonicid_ashwin_3}~\citep{bellur2012knowledge}. Note that \acrshort{tonicid_sengupta}~\citep{Sengupta2005b} was not available for
evaluation and \acrshort{tonicid_chordia} was not available when the experiments were conducted. 

Each of the method mentioned above is evaluated using six different datasets, denoted \acrshort{tds_cm1}, \acrshort{tds_cm2}, \acrshort{tds_cm3}, \acrshort{tds_iitm1}, \acrshort{tds_iitm2} and \acrshort{tds_iisc}. A detailed description of these datasets in terms the musical material and the audio quality is given in~\secref{sec:corpus_tonic_datasets}. A quick summary of the datasets can be obtained from~\tabref{tab:tonic_datasets}. The diversities present in real-world music collections of \gls{iam} in terms of the varied musical and acoustical characteristics are well captured in these six datasets. Note that \acrshort{tonicid_ashwin_1} requires several excerpts from the same concert in order to compute the segmented \gls{gd} histogram, and this kind of data (and metadata) is only available for \acrshort{tds_iitm1} dataset. Hence, \acrshort{tonicid_ashwin_1} is only evaluated using \acrshort{tds_iitm1} dataset.

For vocal performances we evaluate the accuracy of correctly identifying the tonic pitch, whereas for instrumental music we evaluate the accuracy of estimating the tonic pitch-class only (i.e.~the identified tonic pitch is allowed to be in any octave). This is because whilst for vocal music the concept of the tonic pitch being in a specific octave is clearly defined (because it is restricted by the pitch range of the singer), this notion is not as clear for instrumental music. For vocal performances, the tonic identified by a method is considered correct if it is within 50 cents of the ground truth annotation. For instrumental music, a method's estimate is considered correct if it is within 50 cents of the correct tonic pitch-class.

Classification-based approaches, which require training (\acrshort{tonicid_justin} and \acrshort{tonicid_sankalp}) are evaluated by performing 10-fold cross-validation on every dataset, repeating
every experiment 10 times and reporting the mean accuracy over the 10 repetitions. All parameters are kept fixed for all methods across all datasets. Since tonic pitch range for male and female singers, and for instrumental music is different, editorial metadata regarding the gender of the singer and the type of music excerpt (vocal or instrumental) can be used to improve the accuracy of tonic identification. We therefore perform two sets of experiments, one with only using the audio excerpts, and the other in which the methods are also given information about the gender of the singer and the type of excerpt (vocal or instrumental). 

\subsection{Results, Discussion and Summary}
\label{sec:pre_processing_tonic_identification_results}

In this section we present the results of the comparative evaluation. We compare the performance of different tonic identification approaches, highlight their shortcomings and discuss various types of errors made by them. The section is divided into three parts: in ~\secref{sec:pre_processing_tonic_id_results_only_audio_data} we present the results obtained when only the audio data is used and no additional metadata is provided to the methods. Subsequently, we report the performance accuracy
obtained when information regarding the gender of the singer (male or female) and performance type (instrumental or vocal) is provided to the methods in addition to the audio data (\secref{Results obtained using metadata together with the audio}). Finally in \secref{Error Analysis} we present an analysis of the most common errors made by the methods and make some general observations regarding their performances.

\subsubsection{Results obtained using only audio data}
\label{sec:pre_processing_tonic_id_results_only_audio_data}


\subsubsection{Overall results}
In Table~\ref{tab:PerformanceAccuracy_Without_MF_Info} we summarize the
identification accuracies (in percentage) for tonic pitch (TP) and tonic
pitch-class (TPC) obtained by seven methods on six datasets, using
only audio data.

{\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{10pt}
\begin{sidewaystable}
	\centering
	\begin{tabular}{ c | c  c : c  c : c  c : c  c : c  c : c  c }
\tabletop
		\multirow{2}{*}{Methods}  & \multicolumn{2}{ c: }{\acrshort{tds_cm1}} & \multicolumn{2}{ c: }{\acrshort{tds_cm2}} & \multicolumn{2}{ c: }{\acrshort{tds_cm3}}  &    \multicolumn{2}{ c: }{\acrshort{tds_iisc}}  &  \multicolumn{2}{ c: }{\acrshort{tds_iitm1}}  & \multicolumn{2}{ c}{\acrshort{tds_iitm2}} \\
		\cline{2-13}
		{} & TP & TPC    & TP & TPC & TP & TPC & TP & TPC
		& TP & TPC & TP & TPC   \\
\tablemid
		\acrshort{tonicid_justin} & - & 88.9 & 87.4 & 90.1 & \textbf{88.4} & \textbf{91} & 75.6 & 77.5 & 	89.5 & \textbf{97.4} & 90.8 & \textbf{94.1}\\

		\acrshort{tonicid_sankalp} & - & \textbf{92.2} & \textbf{87.8} & \textbf{90.9} & 87.7 & 90.5 &
		79.8 & \textbf{85.3} & \textbf{97.4} & \textbf{97.4} & \textbf{93.6}
		& 93.6  \\
		
		\hdashline
		
		\acrshort{tonicid_ranjani_1} & - & 81.4 & 69.6 & 84.9 & 73.2 & 90.8 & 81.8 & 83.6 & 92.1 & \textbf{97.4} &
		80.2 & 86.9  \\
		
		\acrshort{tonicid_ranjani_2} & - & 63.2 & 65.7 & 78.2 & 68.5 & 83.5 & \textbf{83.6} & 83.6 & 94.7 &
		\textbf{97.4} & 83.8 & 88.8\\
		
		\acrshort{tonicid_ashwin_1} & - & - & - & - & - & - & - & - & 89.5 & 89.5 & - & - \\
		
		\acrshort{tonicid_ashwin_2} & - & 88.9 & 74.5 & 82.9 & 78.5 & 83.4 & 72.7 & 76.4 & 92.1 & 92.1 & 86.6   & 89.1 \\
		
		\acrshort{tonicid_ashwin_3} & - & 86 & 61.1 & 80.5 & 67.8 & 79.9 & 72.7 & 72.7 & 94.7 & 94.7 & 85  & 86.6 \\
\tablebot
	\end{tabular}
	\caption[Tonic identification accuracies of seven methods on six different datasets. These accuracies are when no metadata is used in addition to audio data.]{Accuracies for tonic pitch (TP \%) and  tonic
		pitch-class (TPC \%) identification by seven methods on six different
		datasets using only audio data. The best accuracy obtained for each dataset is
		highlighted using bold text. The dashed horizontal line divides the
		methods based on supervised learning (JS and SG) and those based on expert
		knowledge (RH1, RH2, AB1, AB2 and AB3). TP column for CM1 is marked as `-', because it consists of only instrumental excerpts for which we not evaluate tonic pitch accuracy.}
	\label{tab:PerformanceAccuracy_Without_MF_Info}
\end{sidewaystable}

From \tabref{Table~\ref{tab:PerformanceAccuracy_Without_MF_Info}} we see that most of the methods perform well on all datasets, and the accuracy of the best performing method on each dataset ranges from 84-97\%. We note that the identification accuracy obtained for instrumental music (\acrshort{tds_cm1})
by each method is comparable to the accuracy obtained for vocal music, meaning
the approaches are equally suitable for vocal and instrumental music. The
approaches based on multi-pitch analysis and classification (\acrshort{tonicid_justing} and
\acrshort{tonicid_sankalp}) are more consistent and generally perform better across different
datasets compared to the approaches based only on the predominant pitch (with the
exception of IISCB1, most likely due its poor recording quality). Within the
multi-pitch based approaches, \acrshort{tonicid_sankalp} obtains slightly better identification accuracy
than \acrshort{tonicid_justin}. This is most likely due to the additional predominant melody information used in \acrshort{tonicid_sankalp}, and indeed the difference between the two approaches is
mainly in the TP measure and less so in the TPC measure (i.e.~correctly
identifying the octave of the tonic pitch). As could be expected, the simple maximum peak selection approach
employed by \acrshort{tonicid_ashwin_1} and \acrshort{tonicid_ashwin_3} is too simplistic and the template
matching approach employed in \acrshort{tonicid_ashwin_2} yields better results in most cases. 


SG obtains the best results for the instrumental dataset CM1, with AB2
and JS reporting comparable accuracies. For the CM2 and CM3 datasets, we see
that the multi-pitch based approaches (SG and JS) obtain the best
performance, whilst the predominant pitch based methods exhibit a considerable difference
between the TP and TPC accuracies. This means that in many cases these approaches are
able to identify the tonic pitch-class correctly but fail to identify the
correct octave of the tonic pitch. In the case of RH1, RH2, AB2 and AB3, this
can be attributed primarily to the tonic selection procedure employed by
these approaches. The group-delay processing used in AB2 and AB3, and the
estimators used in RH1 and RH2, accentuate the peaks corresponding to all svars
which have a low degree of pitch variance. This includes both the lower and
higher octave \gls{shadja} and panchama in addition to the middle octave \gls{shadja} (the tonic pitch). Furthermore, the
magnitude of peaks corresponding to \gls{shadja} in higher and lower octave is
sometimes further accentuated by pitch halving and doubling errors produced by
the pitch extraction algorithm. This makes identification of the correct tonic octave
more difficult and as seen in Table~\ref{tab:PerformanceAccuracy_Without_MF_Info}, results in a higher degree of
octave errors.

When considering the results for the IISCB1 dataset, we note that the
performance drops for all methods. The main reason for this is the poor audio
quality of the excerpts in this collection. The recordings are relatively old
and noisy, and contain a humming sound in the background. This makes pitch
tracking very difficult. Furthermore, the drone sound in the recordings is very weak compared to the lead artist, which explains the drop in performance for the multi-pitch based approaches.

If we consider performance for IITM1 on the other hand, we see that all methods
perform very well. This is because each excerpt in this dataset is a full
concert, which includes many performances in different \glspl{raga}. Usually different
set of svars are used in different performances, but with the same tonic pitch
throughout the concert. As a result, the melody histogram contains a very high
peak corresponding to the Sa svar, making it considerably easier to identify the
tonic pitch.
%
% On IITM2 database, JS and SG methods give a high
% accuracy. RH2 and AB2 are comparable in accuracies, while RH1 and AB3
% report similar performances \comjustin{Again, just describing the numbers from
% the table for IITM2 is meaningless. Either we explain WHY we observe these
% results, or we should remove these two sentences}.

\subsubsection{Accuracy as a function of excerpt duration}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch05_preprocessing/figures/Accuracy_Length.pdf}
	\end{center}
	\caption{Accuracy (\%) of different methods on four datasets arranged
		by increasing order of mean duration.}
	\label{fig:tonic_id_accuracy_withoutMF}
\end{figure}


As shown in Table~\ref{tab:Database_MTG}, different datasets contain audio
excerpts of different lengths. In order to investigate a possible 
correlation between the accuracy of a method and the length of an audio
excerpt, in Figure~\ref{fig:AccuracyLength_WithoutMF} we plot the
identification accuracies of the different methods for four of the six datasets ordered
by the mean duration of the excerpts: CM2 (3 min), CM3 (full song), IITM2 (full
song) and IITM1 (full concert). CM1 and IISCB1 are excluded because the
characteristics of these datasets are very different compared to the rest of the
datasets (CM1 contains only instrumental performances and IISCB1 has poor
quality audio). As could be expected, we note that practically for all
methods there is an improvement in the performance as we increase the duration
of the excerpts. Interestingly, the improvement is very significant for
the predominant pitch based methods (RH1, RH2, AB2 and AB3) compared to the multi-pitch
based methods (JS and SG). This implies that the latter approaches, which exploit the
pitch information of the drone instrument, are more robust to the duration of audio data.

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch05_preprocessing/figures/Category_Performance.pdf}
	\end{center}
	\caption{Accuracy (\%) as a function of different attributes
		(Hindustani, Carnatic, male, female).}
	\label{fig:tonic_id_categorywise_performance}
\end{figure}

\subsubsection{Accuracy as a function of excerpt characteristics}
In addition to analyzing the performance accuracy for the whole dataset, we also examine the results as a function of different attributes of
the audio excerpts, namely music tradition (Hindustani or Carnatic) and the
gender of the lead singer (male or female). For this analysis we use the CM2
dataset, as it has the most balanced representation of excerpts from the
different categories. In Figure~\ref{fig:CategoryPerformance} we show the
accuracies obtained by the different methods as a function of the different
attributes. We see that the performance of the multi-pitch based approaches (JS
and SG) is relatively independent of the music tradition (Hindustani or
Carnatic). On the other hand, for the predominant pitch based approaches there is a significant difference in performance for Hindustani and Carnatic music (they obtain considerably better results on Carnatic music). The most notable
difference for these approach is the increased amount of octave errors
made for Hindustani music compared to Carnatic music. 
%
% The reason might be due
% the prominence of tānpūrā in Hindustani recordings, pitch of which is frequently
% registered in the silence regions. Depending on the tuning of the tānpūrā and
% amount of pitch halving errors, ṣadja in the lower octave was found to dominate
% in AB2 and AB3 due to which tonic pitch-class was found to be more accurate when
% compared to Tonic pitch. On identifying the tonic pitch by restricting peak
% picking to the middle octave range, the dominance of the non ṣadja note of the
% tānpūrā was found to affect the accuracy of tonic identification in Hindustani
% more when compared to Carnatic music. Similarly, in case of RH1 and RH2,
% inspection of the contours obtained from Hindustani recordings show a
% concentration of density function in the lower octave regions than in the middle
% or higher octave. Choosing a peak to minimize ṣadja variance to weight ratio,
% leads to  solutions that are off-by-octave. This is manifested as the noticeable

A possible reason for this is that in the Hindustani recordings the \gls{tanpura} is
generally more salient compared to the Carnatic recordings. This results in the
monophonic pitch estimators tracking the \gls{tanpura} in some frames, in particular
when the lead artist is not singing. As a result the pitch histogram includes
high peaks at octave multiples or sub-multiples of the correct tonic pitch. In
the case of AB2, AB3, RH1 and RH2, most octave errors were found to
be sub-multiples of the tonic pitch, caused by the stable and salient lower Sa
played by the drone instrument. 

Now we turn to examine the performance as a function of the gender of the lead
artist (male or female). We see that in general, all approaches perform better
on performances by male singers compared to those by female singers. As in the
case of Hindustani versus Carnatic music, the difference is once again
considerably more significant for the predominant pitch based methods, which make a lot of
octave errors for performances by female singers. As noted earlier, in methods
RH1, RH2, AB2 and AB3 a range of 100-250 Hz is considered for finding the tonic
pitch when no additional metadata about the artists is available. In the case of
female singers, the tonic usually resides in the higher end of this range.
However, the presence of the drone, the tonal sounds produced by percussive
instruments and the octave errors produced by the pitch tracker, all contribute to
the appearance of a high peak one octave below the tonic of the female singer.
This is especially the case for 3-minute excerpts where a limited amount of
vocal pitch information is available.
%
% For multi-pitch classification based methods this
% can be due to the dominance of male singers in the dataset and the frequency
% range that is considered for extracting tonic candidates. Automatically
% learned set of rules to select correct tonic candidate are such that they
% maximise the performance for majority cases (i.e. male singer performances).
% Moreover, since the male singers dominate the dataset, even the frequency range
% selected is biased towards the male singers, to obtain a high overall accuracy.
In the case of the approaches based on multi-pitch analysis and classification (JS and SG),
a probable reason for obtaining better performance for male singers is the
larger amount of excerpts with male singers in the database. As a result, it
is possible that the rules learned by the classifier are slightly biased towards
the performances of male singers.


\subsection{Results obtained using metadata together with the audio}
\label{Results obtained using metadata together with the audio}
In order to reduce the amount of octave errors caused by the
different tonic pitch ranges used by male and female singers, one
can use additional information regarding the gender of the singer (when
available) to guide the method, usually by adjusting the frequency range
considered for the tonic pitch. In this section we analyze the effect of
including information regarding the gender of the singer and the performance
type (vocal or instrumental) on the identification accuracy obtained by the different
methods.

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{centering}
	\begin{tabular}{ c | c  c  c  c  c  c }
\tabletop
		{Methods}  & \acrshort{tds_cm1} & \acrshort{tds_cm2} & \acrshort{tds_cm3} &	\acrshort{tds_iisc} & \acrshort{tds_iitm1} & \acrshort{tds_iitm2}\\
\tablemid
		\acrshort{tonicid_justin} & 88.9 & \textbf{93.6} & 92.4 & 80.9 & \textbf{97.4} & 92.3 \\
		
		\acrshort{tonicid_sankalp} & 92.2 & 90.9 & 90.5 & 85.3 & \textbf{97.4} & \textbf{93.6}  \\
		\hdashline
		\acrshort{tonicid_ranjani_1} & 87.7 & 83.5 & 88.9 & \textbf{87.3} &\textbf{ 97.4} & 91.7 \\
		
		\acrshort{tonicid_ranjani_2} & 79.55 & 76.3 & 82 & 85.5 & \textbf{97.4} & 91.5 \\
		
		\acrshort{tonicid_ashwin_1} & - & - &- & - & \textbf{97.4} & - \\
		
		\acrshort{tonicid_ashwin_2} & \textbf{92.3} & 91.5 & \textbf{94.2} & 81.8 & \textbf{97.4} & 91.1 \\
		
		\acrshort{tonicid_ashwin_3} & 87.5 & 86.7 & 90.9 & 81.8 & \textbf{94.7} & 89.9 \\
\tablebot		
	\end{tabular}
\par	\end{centering}
	\caption{Accuracies (tonic pitch-class (\%)) when using additional
		information regarding the gender of the lead singer (male/female) and
		performance type (vocal/instrumental). The dashed horizontal line divides the
		methods based on supervised learning (JS and SG) and those based on
		expert knowledge (RH1, RH2, AB1, AB2 and AB3).}
	\label{tab:PerformanceAccuracy_With_MF_Info}
\end{table}

In Table~\ref{tab:PerformanceAccuracy_With_MF_Info} we present the
identification accuracies obtained when gender information (male/female) and
performance type (vocal/instrumental) is available to the methods in addition
to the audio data. Note for this evaluation we only report the tonic pitch
accuracy for vocal excerpts (and not pitch-class accuracy) since when this
metadata is available the pitch range of the tonic is known and limited to a
single octave, meaning the TP and TPC accuracies will be the same.

Comparing the results with those in Table~\ref{tab:PerformanceAccuracy_Without_MF_Info} we see that the identification
accuracies for all methods are higher when gender and performance metadata is
available. With the additional information the performance of the predominant pitch based
approaches (AB2, AB3 and RH1) becomes closer to that of the multi-pitch based
approaches (JS and SG). Whilst the performance of all methods is improved, the
increase in accuracy is more considerable for the predominant pitch based approaches which
use template matching (in particular AB2 and AB3) compared to
classification-based approaches (JS and SG). This can be attributed to the fact
that the rules learned automatically using machine learning are more complete
compared to the relatively simple Sa-Pa templates, meaning that the classification
based approaches can correctly identify the octave of the tonic even without
using gender metadata. That is, since both male and female excerpts are used
during training, the influence of the gender of the singer on the pitch
features is implicitly learned by the classifier, thus producing rules that can
handle both male and female performances, even without explicit metadata about
the gender of the singer. On the other hand, manually defined template-based
approaches require this extra information to fine-tune the frequency range
considered for the tonic, after which they obtain comparable performance to
that of the classification-based methods.

A potential advantage of the template-based approaches is that they do not
require training. This, in theory, could make them more generalizable compared
to the classification-based methods. To assess this, we ran an experiment in
which the classification-based approaches were trained on one dataset
and tested on a different dataset (CM2 and IITM2). We found that the results
only went down by approximately 2\% compared to the results obtained using
10-fold cross validation on a single dataset. Furthermore, the datasets used for
this experiment contained relatively different music material (percentage of
Carnatic music excerpts and length of the audio files). This suggests that for
tonic identification the rules learned by the classification-based approaches
are generalizable and can be used to obtain high identification accuracies for
the different types of excerpts evaluated in this study.

\subsection{Error Analysis}
\label{Error Analysis}

We now turn to analyze the different types of errors made by the methods, both
with and without using additional metadata for each dataset. Three common
types of errors were identified: Pa errors, where the fifth (Pa) is selected
instead of the tonic, Ma errors, where the fourth (Ma) is selected instead of
the tonic, and the previously mentioned octave errors, where the correct pitch
is identified but in the wrong octave (usually one octave above or below the
tonic). Since octave errors are already discussed at length in the previous section, here
we focus on all other types of errors, which we divide into three categories: Pa
(for Pa errors), Ma (for Ma errors) and ``Other'', which includes all errors
that are neither Pa, Ma nor octave errors (e.g.~selecting the seventh (\acrshort{ni})
instead of the tonic Sa).

\subsubsection{Errors when only audio data is used}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch05_preprocessing/figures/ErrorAnalysis_Without_MF.pdf}
	\end{center}
	\caption{Percentage of excerpts containing each of the three different
		categories of errors (excluding octave errors): Pa, Ma and Other, when no additional metadata is used.}
	\label{fig:Errors_Without_MF}
\end{figure}

In Figure~\ref{fig:Errors_Without_MF} for each dataset we present the
percentage of excerpts containing each of the three categories of errors for
every method (when no additional metadata is used).
% Note that we don't report percentage of the octave errors explicitly because
% these can be inferred from Table \ref{tab:PerformanceAccuracy_Without_MF_Info}
% as the difference between accuracy of tonic pitch and pitch-class.
We see that for most datasets Pa and Ma errors constitute a large proportion of
the total amount of errors made by each method. These confusions make sense from a
musical perspective, since in every Indian art music performance one of
these two svars (Pa or Ma) is always present in the melody in addition to Sa
(the tonic pitch-class). Furthermore, the pitch distance between Sa and Pa (fifth) is
the same as the distance between Ma and higher Sa, and the pitch distance
between Sa and Ma (one fourth) is same as the distance between between Pa and higher Sa.
Since most approaches are based on templates or rules that consider the pitch
distance between the peaks of the feature histogram, these equivalences can
cause four types of confusions: considering a Sa-Pa pair to be Ma-Sa leading to
a Pa error, considering Ma-Sa to be Sa-Pa leading to a Ma error, considering
Sa-Ma to be Pa-Sa leading to a Ma error and considering Pa-Sa to be Sa-Ma
leading to a Pa error.

For the approaches based on multi-pitch analysis (JS and SG) we observe that the
only case where we get more `Other' errors compared Pa and Ma errors is for the
IISCB1 dataset. Since the drone sound is very weak in the excerpts of this
dataset, there are cases in which the prominent peaks of the multi-pitch
histogram correspond to svars other than Sa, Ma and Pa (which will depend on the
choice of the \gls{raga}). Since these approaches assume that the multi-pitch histogram
represents the svars of the drone instrument, the peaks of the
histogram are mistakenly identified as Sa and Pa or Sa and Ma, leading to an
error in identification. For these specific type of excerpts the RH1 method
produces slightly better results, as the \gls{shadja} is not inflected (i.e. there is
little pitch variation) regardless of the \gls{raga}.

In many cases we observe that the percentage of Ma errors is greater than the
percentage of Pa errors. For the classification based approaches, this can be
attributed to the fact that in most excerpts the drone instrument is tuned to
\textit{Pa tuning} (lower Sa, middle Sa, middle Sa, lower Pa). This creates a
bias in the training and the rules learned by the classifier work better for Pa
tuning. Ma errors are also common in RH2, as the estimator looks for a
Sa-Pa-higher Sa pitch relation, which would also fit a Ma-tuned performance.
RH1 on the other hand does not search for a Sa-Pa-Sa template, resulting
in a low proportion of Ma errors compared to the other methods. Finally we note
that most methods do not make any Ma errors on the IITM1 dataset. This is
because the items in this dataset are full concerts, each concert
consisting of several pieces. Whilst Ma may be included in the melody of some of
the pieces, Pa and Sa are always present. As a result, the pitch histogram for
the complete concert does not contain a prominent Ma peak, meaning that it is highly
unlikely for it to be selected as the tonic.

\subsubsection{Errors when metadata is provided}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch05_preprocessing/figures/ErrorAnalysis_With_MF.pdf}
	\end{center}
	\caption{Percentage of different type of errors (Pa, Ma  and Others) by different methods on all the datasets using information regarding the gender of the singer and performance type}
	\label{fig:Errors_With_MF}
\end{figure}

We examine how the errors are affected once we allow methods to use
gender and performance metadata, shown in Figure~\ref{fig:Errors_With_MF}.
If we compare the results to those of Figure~\ref{fig:Errors_Without_MF}, we see
that Ma and Pa errors are reduced more than ``Other'' errors. By
restricting the tonic frequency range to a single octave we prevent the
appearance of a high Sa peak, thus avoiding the possible confusion
between fourths and fifths explained earlier and reducing the amount of Pa and
Ma errors.

For RH1 and RH2 the percentage of Ma errors actually increases slightly after
including male/female information. A large proportion of these errors were
observed in excerpts with female singers. For these excerpts, the range
for \gls{shadja} candidates is limited to 130-250 Hz. For this range, candidates
fitting a lower Ma-middle Sa-middle Ma template would also satisfy the
minimization criteria used in RH2. In the case of RH1, the reduced frequency
range results in relatively weak peaks also being considered, and their small
pitch variance can result in the wrong candidate being selected
during the minimization process.

\subsubsection{Errors as a function of excerpt attributes}


\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch05_preprocessing/figures/Category_Errors.pdf}
	\end{center}
	\caption{Percentage
		of excerpts with the different categories of errors
		(Pa, Ma and Others) for every method as a function of different
		excerpt attributes (Hindustani, Carnatic, male, female).}
	\label{fig:CategoryErrors}
\end{figure}

Finally, we analyze the errors as a function of the different attributes of the
excerpts (Hindustani versus Carnatic, male versus female). As in Section
\ref{Results obtained using only audio data}, we use the CM2 dataset for the
analysis because it is the most balanced, and methods are not provided with
any metadata in addition to the audio signal. The percentage of excerpts
containing each of the three categories of errors (Pa, Ma and Other) for
every approach as a function of the different excerpt attributes is
shown in Figure~\ref{fig:CategoryErrors}.
%
We see that for the classification based methods, the proportion of Ma errors is
much higher in performances by female singers compared to performances by male
singers.
% This is because both the female performances and the Ma tuning cases are less
% in number compared to the male performances and the Pa tuning cases.
The pitch range of the tonic for female singers is such that the lower Ma
resides in the frequency range where the tonic of most male singers lies. Thus,
the lower Ma-middle Sa (fifth) relationship for female singers is often confused
with middle Sa-Pa relationship for male singers, resulting in a high number of
Ma errors. 
% For methods AB2 and AB3 we observe that the percentage of 'Other'
% type of errors is exceptionally high for Hindustani music and performances of
% female singers.



For further details and insights regarding the types of error made by the
different methods and their underlying causes we refer the reader to the
publications where these methods are discussed in depth
\cite{SalamonSankalp2012, SGulati_MThesis2012, Ranjani2011,
	Ashwin_Istanbul2012}.



\subsection{Correcting Mistakes in Tonic Identification}
\label{sec:pre_processing_tonic_identification_correcting_mistakes}









\section{Melody Estimation, Post-processing and Representation}
\label{sec:data_preprocessing_melody_estimation}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeEighty]{ch05_preprocessing/figures/figure_todo.pdf}
	\end{center}
	\caption{Example of a continuous pitch contour corresponding to the predominant melodic source.}
	\label{fig:predominant_melodic_fragment}
\end{figure}


\subsection{Predominant Melody Estimation}
\label{sec:data_preprocessing_predominant_melody_estimation}

As mentioned before, the primary aim in this thesis is to develop computational models to analyze the melodic aspects of \gls{iam}. It is therefore essential to clearly understand  the engineering definition of melody that we consider in this thesis. \Gls{iam} is heterophonic in nature (Section~\TODO{XXX}), wherein there are typically two melodic lines or melodic sources; the primary source, which is the lead artist and the secondary source, which is the accompanying instrumentalist. In this thesis we consider melody as a continuous pitch contour corresponding to the predominant melodic source (typically, the lead artist) in the audio signal. An example of a melody of an audio excerpt is shown in Figure~\ref{fig:predominant_melodic_fragment}, where the contour represents the pitch of the lead singer at each instance in time. Usage of such a melody representation is a common practice, done by several other researchers for a variety of melody processing tasks across different music traditions~\citep{Dutta2014,Ishwar2013,Rao2014,koduri2014intonation,senturk2013score,pikrakis2012tracking}. \TODO{Add papers of other music genres for diff type of tasks} For other interesting definitions of melody we refer the readers to~\citep{Salamon2012}. 


Pitch estimation, also commonly known as pitch tracking from audio signals has been an active research topic since several decades. As indicated in Section~\TODO{XXX} the challenges in the estimation of a reliable pitch contour differ across the type of audio music signals (monophonic or polyphonic). As a result of which the performance of these algorithms vary significantly across different music genres, which also seems to be correlated with the extent of the polyphony in the music. A sparse polyphonic characteristic in audio recordings of \gls{iam} due to its heterophonic nature lessens the complexity of the task to an extent compared to the music genres such as death-metal or orchestral music. This trend is clearly visible from the past MIREX results\footnote{http://www.music-ir.org/mirex/wiki/MIREX\_HOME}. Compare the accuracy obtained by the best performing method on the \gls{iam} data set with the rest of the data sets.

In order to estimate the pitch of the predominant melodic source in the audio recordings of \gls{iam} we use Melodia algorithm, a state-of-the-art melody extraction method proposed by~\cite{Salamon2012}. This method performed favorably in MIREX~2011 (an international MIR evaluation campaign) on a variety of music genres, including \gls{iam}. This method is used in several other studies that analyze melodies extracted from audio signals~\citep{Dutta2014,Ishwar2013,Rao2014,koduri2014intonation,senturk2013score,pikrakis2012tracking}. %\TODO{Another motivation is that this function works on continuous contour basis, so even if the classification of contour goes wrong there is a high chance that we have obtained the contour corresponding to the violin phrase. Since that also follows the melody there are several times melodic phrases obtained in the violin voice.}

Currently, there are two implementations of Melodia algorithm available for researchers to use. We use the implementation as available in Essentia~\citep{essentia}. Essentia\footnote{https://github.com/MTG/essentia} is an open-source C++ library for audio analysis and content-based MIR. We use the default values of the parameters, except for the frame and hop sizes, which are set to 46 and 2.9\,ms, respectively. The other implementation of Melodia is available as a Vamp plugin\footnote{http://mtg.upf.edu/technologies/melodia}. 

Prior to the predominant pitch estimation we apply an equal-loudness filter available in Essentia to make the processing perceptually relevant~\TODO{ref}. For this we use the default set of parameters. Noticeably, the predominant pitch estimation algorithm also performs voicing detection. This means that the algorithm in addition to estimating the pitch also detects the time segments where the predominant voice is absent. Such time segments where the predominant melodic source is inactive are assigned a dummy pitch value (usually 0). \TODO{rephrase this part}.

All the methods reported in this dissertation mainly utilize the pitch or the tonal aspects of melody. However, for the tasks considered in this work such as melodic similarity, usage of loudness and timbral aspects of melody might be worth exploring. These features largely capture the phonetics related aspects and might be important in studying artist specific nuances in renderings of melodic phrases. To get an idea about the type of information captured by these features and their usefulness we take an example here. We synthesize the harmonic series corresponding to a voice extracted from an excerpt of Carnatic music. During the synthesis we force the pitch of the voice to a monotone while keeping the other aspects (loudness and timbre) intact. The original excerpt\footnote{original link}, extracted predominant pitch\footnote{pitch synthesizes} and the synthesized monotone voice\footnote{monotone sunthesis} is available for listening on Freesound. In addition, we also show the spectogram of the original excerpt and the monotone synthesis in Figure~\TODO{} After listening to this example we get an idea about the amount of useful information that is left behind if we just consider the pitch aspect of the melody in the analysis. Exploiting loudness and timbral aspects for melodic analysis is left for our future work.

\TODO{Obtaining pitch tracks using Dunya? should we give that info in each feature or just at once we show it for all? Maybe the specific version that we have used can be given in specific sections}
%Thoughts
%\begin{itemize}
%	\item What did we want to extract from the audio for the purpose of this thesis work. What are the dimensions of the melody that we don't consider. 
%	\item how is that fullfilled by predominant melody
%	\item Caution to the readers about the usage of the word melody.
%	\item what do we do to extract predominant melody. This problem being simpler compared to the western popular music genres.
%	\item parameter settings
%	\item motivation to choose this algorithm
%	\item How can one use it, have we shared our melody representations?
%	\item Sampling rate depends on the task, hence we talk about that when we talk about the task
%\end{itemize}

\subsection{Pitch Post-processing}
\label{sec:data_preprocessing_pitch_postprocessing}

Predominant pitch estimation is not yet a solved problem and the output pitch contour is still far from being perfect. While these algorithms get better over years, a number of errors in predominant pitch estimation can be alleviated by post-processing the pitch contour. During the post-processing we perform octave correction and smoothening operations on the pitch contour extracted as mentioned above. In addition to these two operations, at times we interpolate the unvoiced regions in the melody, which correspond to the short breath pauses taken by the artists. In subsequent sections we describe these post-processing steps in detail.

\subsubsection{Correcting Spurious Pitch Jumps}
\label{sec:data_processing_correcting_pitch_jumps}

One of the most frequently occurring errors in pitch estimation is detecting pitch in the wrong octave, commonly referred to as an octave error. Identifying and correcting such an error during a post-processing step for the case of a monophonic signal (containing a single harmonic series) is fairly easy\TODO{reference}. One can compare the energy of the partials with the total energy of the audio frame to infer if the estimated pitch value corresponds to the actual fundamental frequency. However, for the case of a polyphonic signal the task becomes much more challenging. 


We alleviate this problem to an extent by restricting ourselves to a specific set of pitch octave errors occurring over short duration of time. Instead of identifying a pitch octave error for an individual audio frame, which as we mentioned is a challenging task for a polyphonic audio signal, we exploit the temporal continuity of melody to detect such errors. We explain the intuition behind our method with the help of an example shown in Figure~\TODO{fig}. The ground truth pitch contour is shown in XXX and the estimated contour in XXX. If we analyze an audio frame between time XX and XX seconds, automatically identifying if there is an pitch octave error or not is a challenging task. However, analyzing the pitch continuity we can easily detect an anomalous pitch jump at time XX seconds. This is due to the fact in a real-world scenario such drastic pitch jumps do not occur. By analyzing the amount of frequency difference across the pitch transition we can infer to an extent the type of pitch error and can subsequently correct it. In this example the frequency jump at time XX seconds is XXX cents. Knowing that there is another jump in the other direction at time XX makes it more probably that the pitch segment between time XX and XX seconds suffers from an octave error. 

We now describe our approach more formally. 











\subsubsection{Pitch smoothening}
\label{sec:data_processing_pitch_smoothening}


\subsubsection{Pitch Interpolation}
\label{sec:data_processing_pitch_interpolation}



 

We proceed to post-process the pitch contours and remove the spurious pitch jumps lasting over a few frames as well as smooth the pitch contours. We first apply a median filter over a window size of 50\,ms, followed by a low-pass filter using a Gaussian window. The window size and the standard deviation of the Gaussian window is set to 50\,ms and 10\,ms, respectively. The pitch contours are finally down-sampled to 100\,Hz, which was found to be an optimal sampling rate in~\cite{gulati_ICASSP2015}. 

\begin{itemize}
	\item What are the errors in pitch estimation that we face
	\item Post processing to correct them. What are the steps that we do.
	\item Examples of post processed pitch contours.
\end{itemize}


\section{Pitch Representation} 

For the pitch representation to be musically relevant, the pitch values are converted from Hertz to Cents (logarithmic scale). For this conversion we additionally consider the tonic pitch of the lead artist as the reference frequency (i.e.,~0\,Cent corresponds to the tonic pitch). Thus, our representation becomes independent of the tonic of the lead artist, which allows a meaningful comparison of melodies of two distinct recordings (even if sung by two different artists in different tonic pitches). The tonic of the lead artist is identified automatically using a classification-based multi-pitch approach~\cite{Gulati2014Tonic}. We use the implementation of this method available in Essentia with the default set of parameters.%\vspace{0.5em}


\section{Nyas identification}


%\section{Method}
%\label{method}

%\begin{figure}[t]
%	\begin{center}
%		\includegraphics[scale=0.68]{fig/BlockDiagram.pdf}\vspace{-1.5em}
%	\end{center}
%	\caption{Block diagram of the proposed approach.}
%	\label{fig:BD_OverallApproach}
%\end{figure}

\subsection{Predominant Pitch Estimation \& Representation}

The proposed method is comprised of four main blocks (Figure~\ref{fig:BD_OverallApproach}): predominant pitch estimation and representation, segmentation, feature extraction, and segment classification and fusion. For estimating pitch of the the predominant melodic source\footnote{This task is also referred as predominant melody extraction in various contexts within Music Information Research.} we use the method by Salamon~\&~G\'omez~\cite{Salamon2012}, which scored very favorably in an international evaluation campaign featuring a variety of musical genres, including Indian art music\footnote{\url{http://nema.lis.illinois.edu/nema_out/mirex2011/results/ame/indian08/summary.html}}. For the pitch representation to be musically meaningful, we convert the pitch values to cents, normalized by the tonic frequency of the lead artist. Tonic of the lead artist is extracted automatically using the approach proposed by Gulati~\cite{SGulati_MThesis2012}. In a comparative evaluation of different tonic identification approaches for Indian art music, this approach consistently performed better for a variety of music material within Indian art music~\cite{Gulati2013Tonic}. For both predominant pitch estimation and tonic identification we use the implementations available in Essentia~\cite{essentia}, an open-source C++ library for audio analysis and content-based music information retrieval.


\subsection{Segmentation}
\label{sec:segmentation}

Ny\={a}s segment is a rendition of a single svar and the aim of the segmentation process is to detect the svar boundaries. However, svars contain different alank\={a}rs as discussed before where pitch deviation with respect to the mean svar frequency can go roughly up to 200\,cents. This characteristic of a svar in Hindustani music poses a challenge to segmentation. To illustrate this, in Figure~\ref{fig:SegmentationIllustration} we present an example of a ny\={a}s segment (between $T_1-T_9$, centered around mean svar frequency $S_n=990$\,cents). The pitch deviation in this ny\={a}s segment with respect to the mean svar frequency reaches almost 100\,cents (between $T_5-T_6$). Note that the reference frequency, i.e. 0\,cent is the tonic pitch of the lead singer.


%\begin{figure}
%	\includegraphics[scale=0.47]{fig/SegmentationMethod.pdf}\vspace{-0.5em}
%	\caption{Fragment of a pitch contour containing a ny\={a}s segment ($T_1-T_9$), where $T_i$s denote time stamps and $S_n$s denote mean svar frequencies. The pitch deviation within the ny\={a}s segment ($T_1-T_9$) is almost 100\,cents ($T_5-T_6$).}
%	\label{fig:SegmentationIllustration}
%\end{figure}

We experiment with two different methods for segmenting melodies: piece-wise linear segmentation (PLS), a classical, generic approach used for the segmentation of time series data~\cite{keogh2004segmenting}, and our proposed method, which incorporates domain knowledge to facilitate the detection of ny\={a}s boundaries. For PLS we use a bottom-up segmentation strategy as described in~\cite{keogh2004segmenting}. Bottom-up segmentation methods involve computation of residual error incrementally for each sample of time series. When the residual error satisfies a pre-defined criterion a new segment is created. Out of the two typical criteria used for segmentation, namely average and maximum error, we choose the latter because, ideally, a new segment should be created as soon as the melody progresses from one svar to the other. In order to select the optimal value of the allowed maximum error, which we denote by $\epsilon$, we iterated over four different values and chose the one which resulted in the best performance. Specifically, for $\epsilon=\lbrace 10, 25, 50, 75\rbrace$, $\epsilon=75$\,cents yielded the best performance (we rejected $\epsilon\geq 100$\,cents in early experimentation stages because few svars of a r\={a}g are separated by an interval of 100\,cents and, therefore, the segmentation output was clearly unsatisfactory).

To make the segmentation process robust to pitch deviations, we propose a method based on empirically-derived thresholds. Unlike PLS, our proposed method computes a pitch histogram and uses that to estimate mean svar frequencies before the computation of residual error. This allows us to compute the residual error with respect to the mean svar frequency instead of computing it with respect to the previous segment boundary, as done in PLS.  In this way our proposed method utilizes the fact that the time series being segmented is a pitch contour where the values of the time series hover around mean svar frequencies. The mean svar frequencies for an excerpt are estimated as the peaks of the histogram computed from the estimated pitch values. An octave folded pitch histogram is computed using a 10\,cent resolution and subsequently smoothened using a Gaussian window with a variance of 15\,cents. Only the peaks of the normalized pitch histogram which have at least one peak-to-valley ratio greater than 0.01 are considered as svar locations. As peaks and valleys we simply take all local maximas and minimas over the whole histogram. In Figure \ref{fig:pitchHistogram} we show an example of an octave folded normalized  pitch histogram used for estimating mean svar frequencies. The estimated mean svar frequencies are indicated by circles. We notice that the pitch values corresponding to a svar span a frequency region and not a single value.

%\begin{figure}
%	\includegraphics[scale=0.45]{fig/swarOnHistogram.pdf}\vspace{-0.5em}
%	\caption{Normalized octave folded pitch histogram used for estimating mean svar frequencies. Estimated mean svar frequencies are indicated by circles.}
%	\label{fig:pitchHistogram}
%\end{figure}

After we estimate mean frequencies of all the svars in a piece, we proceed with their refinement. For the $n$-th svar $S_n$, we search for contiguous segments within a deviation of $\varepsilon$ from $S_n$, that is, $\vert S_n-P_i \vert < \varepsilon$, for~$i\in[1,N]$, where $P_i$ is the fundamental frequency value (in cents) of the $i$-th sample of a segment of length $N$. In Figure~\ref{fig:SegmentationIllustration}, this corresponds to segments $[T_1,T_2]$, $[T_3,T_4]$, and $[T_7,T_8]$.

Next, we concatenate two segments $[T_a,T_b]$ and $[T_e,T_f]$ if two conditions are met:
\begin{enumerate}
	\item $P_i-S_n < \rho_1$ and $S_n-P_i < \rho_2$, for $i\in[T_b,T_e]$, where $\rho_1 = S_{n+1}-S_n + \varepsilon$ and $\rho_2 = S_n-S_{n-1} + \varepsilon$. 
	\item $T_c-T_d < \delta$, where $\delta$ is a temporal threshold and $[T_c,T_d]$ is a segment between $T_b, T_e$ such that $\vert S_m-P_i\vert <\varepsilon$ for $i\in[T_c,T_d]$ for $m\in [S_{n-1}, S_{n+1}]$ and $m \neq n$.
\end{enumerate}
In simple terms, we concatenate two segments if the fundamental frequency values between them do not deviate a lot (less than $\rho_1$ and $\rho_2$) and the time duration of the melody in close vicinity (less than $\epsilon$) of neighboring svars is not too large (less than $\delta$). We repeat this process for all svar locations. In our experiments, we use $\varepsilon = 25$\,cents and $\delta=50$\,ms, which were empirically obtained. In the example of Figure~\ref{fig:SegmentationIllustration}, we see that the two conditions apply for segments $[T_1,T_2]$ and $[T_3,T_4]$, and not for $[T_3,T_4]$ and $[T_7,T_8]$ because $T_6-T5>\delta$. Notice that we can already derive a simple binary flatness measure $\nu$ for $[T_a, T_b]$, $\nu=1$ if $\vert S_n-P_i \vert< \epsilon$ for $i \in [T_a, T_b]$ for any $n$ and $\nu=0$ otherwise. 

\subsection{Feature Extraction}
\label{sec:features}

We extract musically motivated melodic features for segment classification, which resulted out of discussions with musicians. For every segment, three sets of melodic features are computed: local features~(L), which capture the pitch contour characteristics of the segment, contextual features~(C), which capture the melodic context of the segment, and a third set combining both of them~(L+C) in order to analyze if they complement each other. Initially, we considered 9 local features and 24 contextual features:

\begin{description}
	\item[Local Features:] segment length, mean and variance of the pitch values in a segment, mean and variance of the differences in adjacent peak locations of the pitch sequence in a segment, mean and variance of the  peak amplitudes of the pitch sequence in a segment, temporal centroid of the pitch sequence in a segment normalized by its length, and the above-mentioned flatness measure $\nu$ (we use the average segmentation error for the case of PLS).
	\item[Contextual Features:] segment length normalized by the length of the longest segment within the same breath phrase\footnote{Melody segment between consecutive breaths of a singer. We consider every unvoiced segment (i.e.,~a value of 0 in the pitch sequence) greater than 100\,ms as breath pause.}, segment length normalized by the length of the breath phrase, length normalized with the length of the previous segment, length normalized by the length of the following segment, duration between the ending of the segment and succeeding silence, duration between the starting of the segment and preceding silence, and all the local features of the adjacent segments.
\end{description}

However, after preliminary analysis, we reduced these features to 3 local features and 15 contextual features. As local features we selected length, variance, and flatness measure ($\nu$). As contextual features we selected all of them except the local features of the posterior segment. This feature selection was done manually, performing different preliminary experiments with a subset of the data, using different combinations of features and selecting the ones that yielded the best accuracies.

\subsection{Classification and Segment Fusion}

Each segment obtained in Section~\ref{sec:segmentation} is classified into ny\={a}s or non-ny\={a}s based on the extracted features of Section~\ref{sec:features}. To demonstrate that the predictive power of the considered features is generic and independent of a particular classification scheme, we employ five different algorithms exploiting diverse classification strategies~\cite{Hastie09BOOK}: trees (Tree), $K$ nearest neighbors (KNN), naive Bayes (NB), logistic regression (LR), and support vector machines with a radial basis function kernel (SVM). We use the implementations available in scikit-learn~\cite{scikitlearn}, version 0.14.1. We use the default set of parameters with few exceptions in order to avoid over-fitting and to compensate for the uneven number of instances per class. Specifically, we set \texttt{min\_samples\_split=10} for Tree, \texttt{fit\_prior=False} for NB, \texttt{n\_neighbors=5} for KNN, and for LR and SVM \texttt{class\_weight=`auto'}.

For out-of-sample testing we implement a cross-fold validation procedure. We split the data set into folds that contain an equal number of ny\={a}s segments, the minimum number of ny\={a}s segments in a musical excerpt (7 in our case). Furthermore, we make sure that no instance from the same artist and r\={a}g is used for training and testing in the same fold.

After classification, boundaries of ny\={a}s and non-ny\={a}s segments are obtained by merging all the consecutive segments with the same segment label. During this step, the segments corresponding to the silence regions in the melody, which were removed during classification, are regarded as non-ny\={a}s segments.

\subsection{Experimental Setup}
\label{experimentalSetup}

\subsubsection{Music Collection and Annotations}

The music collection used for evaluation consists of 20 audio music recordings  of total duration of 1.5 hours, all of which are vocal \={a}l\={a}p performances of Hindustani music. \={A}l\={a}ps are unmetered melodic improvisational sections, usually performed as the opening of a raga rendition. We selected only \={a}l\={a}p performances because the concept of ny\={a}s is emphasized in these sections during a r\={a}g rendition. Of the 20 recordings, 15 are polyphonic commercially-available audio recordings compiled as a part of the CompMusic project\footnote{http://compmusic.upf.edu/corpora}~\cite{XavierSerra2011}. The other 5 audio recordings in the data set are monophonic in-house studio recordings of the \={a}l\={a}ps sung by a professional singer of Hindustani music. The in-house audio recordings are available under creative commons (CC) license in Freesound\footnote{http://www.freesound.org/people/sankalp/packs/12292/}. In total we have performances by 8 artists in 16 different r\={a}gs. In order avoid over-fitting of the learned model it is important to include different artists and r\={a}gs as the ny\={a}s characteristics highly depend on these aspects~\cite{Dey2008}.

Ny\={a}s segments were annotated by a performing artist of Hindustani music (vocalist) who has received over 15 years of formal musical training. 
The musician marked all the ny\={a}s segment boundaries and labeled them appropriately. After annotation, we obtained 1257 ny\={a}s svar segments. The duration of these segments vary from 150\,ms to 16.7\,s with a mean of 2.46\,s and median of 1.47\,s.

\subsubsection{Evaluation Measures and Statistical Significance}

For the evaluation of ny\={a}s boundary annotations we use hit rates as in a typical music structure boundary detection task~\cite{Ong05ICMC}. While calculating hit rate, segment boundaries are considered as correct if they fall within a certain threshold of a boundary in the ground-truth annotation. Using matched hits, we compute standard precision, recall, and F-score for every fold and average them over the whole data set. The choice of a threshold however depends on the specific application. Due to the lack of scientific studies on the just noticeable differences of ny\={a}s svar boundaries, we computed results using an arbitrary selected threshold of 100\,ms. Label annotations are evaluated using standard pairwise frame clustering method as described in~\cite{levy2008structural}. Frames with same duration as threshold value for the boundary evaluation (i.e. 100~ms) are considered while computing precision, recall, and F-score. For assessing statistical significance we use the Mann-Whitney U test~\cite{mann1947test} with $p<0.05$ and assuming an asymptotic normal distribution of the evaluation measures. To compensate for multiple comparisons we apply the Holm-Bonferroni method~\cite{holm1979simple}, a powerful method that also controls the so-called family-wise error rate. Thus, we end up using a much more stringent criteria than $p<0.05$ for measuring statistical significance.

\subsubsection{Baselines}

Apart from reporting the accuracies for the proposed method and its variants, we compare against some baseline approaches. In particular, we consider DTW together with a KNN classifier ($K=5$). For every segment, we compute its distance from all other segments and assign a label to it based on the labels of its $K$ nearest neighbors, using majority voting. As the proposed method also exploits contextual information, in order to make the comparison more meaningful, we consider the adjacent segments in the distance computation with linearly interpolated values in the region corresponding to the segment. For comparing with the variant of the proposed method that uses a combination of the local and contextual features, we consider adjacent segments together with the actual segment in the distance computation. As this approach does not consider any features, it will help us in estimating the benefits of extracting musically-relevant features from ny\={a}s segments. 

In addition, to quantify the limitations of the adopted evaluation measures, we compute a few random baselines. The first one (RB1) is calculated by randomly planting boundaries (starting at 0\,s) according to the distribution of inter boundary intervals obtained using the ground-truth annotations. For each segment we assign the labels `ny\={a}s' with a a priory probability (same for all excerpts) computed using ground truth annotations of the whole data set. The second one (RB2) is calculated by planting boundaries (starting at 0\,s) at even intervals of 100\,ms and assigning class labels as in RB1. Finally, the third one (RB3) considers the exact ground-truth boundaries and assigns the class labels randomly as in RB1 and RB2. Thus, with RB3 we can directly assess the impact of the considered classification algorithms. We found that RB2 achieves the best accuracy and therefore, for all the following comparisons we only consider RB2.

%\begin{table} 
%\centering
%\begin{tabular}{ c | c c c }
%\hline\hline
%  		&	RB1	&	RB2	&	RB3\\
%\hline
% 	ny\={a}s Boundary		&  0.10 & 0.17 &1.0  \\ 
%%\hline 	
%	ny\={a}s Region		& 0.56 & 0.69 & 0.64 \\
%\hline\hline
%\end{tabular}
%
%\caption{F-scores for ny\={a}s boundary estimation and ny\={a}s region
%estimation using random baseline methods. \XXX{J}{S}{Why not adding a row or column in the general results tables??}}
%	\label{tab:randombaseline}
%\end{table}

\subsection{Results and Discussion}
\label{ResultsAndDiscussion}


We evaluate two tasks, ny\={a}s segment boundary annotation, and ny\={a}s and non-ny\={a}s segment label annotation. For both the tasks, we report results obtained using two different segmentation methods (PLS and the proposed segmentation method), five classifiers (Tree, KNN, NB, LR, SVM), and three set of features (local (L), contextual(C) and local together with contextual (L+C)). In addition, we report results obtained using a baseline method (DTW) and a random baseline (RB2).


In Table~\ref{tab:accuraciesboundary} we show the results of ny\={a}s boundary annotations. First, we see that every variant performs significantly better than the best random baseline. RB2 yields an F-score of 0.184 while the worst variant tested reaches 0.248. Next, we see that the proposed method achieves a notably higher accuracy compared to the DTW baseline. Such difference is found to be statistically significant, with the only exception of the NB classifier. For a given feature set, the performance differences across classifiers are not statistically significant. The only exceptions are Tree and NB, which yield relatively poor and inconsistent performances over different feature sets. We opted to not consider these two classifiers in the following comparisons. Among  feature sets, the performance differences are not statistically significant between PLS variants (Table~\ref{tab:accuraciesboundary}, top rows), whereas for the case of the proposed segmentation method (Table~\ref{tab:accuraciesboundary}, bottom rows), we find that the local features perform significantly better than the contextual features and their combination does not yield consistent improvements. Finally, we see that the best results are obtained using the proposed segmentation method together with the local features, with a statistically significant difference to its competitors. Furthermore, the worst accuracy obtained using the proposed segmentation method is notably higher than the best accuracy using PLS method, again with a statistically significant difference.


\begin{table} 
	\centering
	\tabcolsep = 0.15cm
	\renewcommand{\arraystretch}{1.15}
	\begin{tabular}{ c|c|c| c c c c c c }
		
		\hline\hline
		& Feat.		&	DTW & Tree	 &	KNN 	&	NB		& LR 	&	SVM\\
		
		\hline
		\multirow{3}{*}{A} & 	L		&  0.356 & 0.407 & 0.447 & 0.248 & 0.449 & 0.453\\ 
		%\hline 	
		&	C		& 0.284 & 0.394 & 0.387 & 0.383 & 0.389 & 0.406 \\
		%\hline	
		&	L+C		& 0.289 & 0.414 & 0.426 & 0.409 &0.432 & 0.437 \\
		\hline  		
		\multirow{3}{*}{B} &	L		& \textbf{0.524} & 0.672 & \textbf{0.719} & 0.491 & \textbf{0.736} & \textbf{0.749}\\ 
		%\hline 	
		&	C		& 0.436 & 0.629 & 0.615 & \textbf{0.641} & 0.621 & 0.673 \\
		%\hline	
		&	L+C		& 0.446 & \textbf{0.682} & 0.708 & 0.591 & 0.725 & 0.735\\  		
		\hline\hline
		
	\end{tabular}
	
	\caption{F-scores for ny\={a}s boundary detection using PLS method (A) and the proposed segmentation method (B). Results are shown for different classifiers (Tree, KNN, NB, LR, SVM) and local (L), contextual (C) and local together with contextual (L+C) features. DTW is the baseline method used for comparison. F-score for the random baseline obtained using RB2 is 0.184. }
	\label{tab:accuraciesboundary}
\end{table}

In Table~\ref{tab:accuraciesRegion} we show the results for ny\={a}s and non-ny\={a}s label annotations. Basically, we can draw similar conclusions as with Table~\ref{tab:accuraciesboundary}: (1) all method variants perform significantly better than the random baselines, (2) all the proposed method variants yield significant accuracy increments over the DTW baseline, and (3) no statistically significant differences between classifiers (with the aforementioned exceptions). In label annotations, unlike the boundary annotations, we find that though the local features perform better than the contextual features, the differences are not statistically significant for all the proposed method variants. Furthermore, we also see that the proposed segmentation method consistently performs  better than PLS. However, the differences are not statistically significant.

In addition, we also investigate per-class accuracies for label annotations. We find that the performance for ny\={a}s segments is considerably better than non-ny\={a}s segments. This could be attributed to the fact that even though the segment classification accuracy is balanced across classes, the differences in segment length of ny\={a}s and non-ny\={a}s segments (ny\={a}s segments being considerably longer than non-ny\={a}s segments) can result in more number of matched pairs for ny\={a}s segments.

\begin{table} 
	\centering
	\tabcolsep = 0.15cm
	\renewcommand{\arraystretch}{1.15}
	\begin{tabular}{ c|c|c | c  c  c  c  c  c }
		\hline\hline
		& Feat.	&	DTW & Tree	 &	KNN 	&	NB		& LR 	&	SVM	\\
		\hline
		
		\multirow{3}{*}{A} &   L		& \textbf{0.553} & 0.685 & 0.723 & 0.621 & 0.727 & 0.722	\\
		%\hline         
		&	C   		& 0.251 & 0.639 & 0.631  & 0.690 & 0.688 & 0.674	\\
		%\hline 
		& 	L+C		& 0.389 & 0.694 & 0.693 & 0.708 & 0.722 & 0.706	\\	
		\hline
		\multirow{3}{*}{B} & 	L		& 0.546 & \textbf{0.708} & \textbf{0.754} & 0.714 & \textbf{0.749} & \textbf{0.758} \\
		%\hdashline 	
		& 	C		&0.281 & 0.671 & 0.611 & 0.697 & 0.689 & 0.697\\
		%\hline	
		& 	L+C		& 0.332 & 0.672 & 0.710 & \textbf{0.730} & 0.743 & 0.731\\
		\hline\hline        
	\end{tabular}
	\caption{F-scores for ny\={a}s and non-ny\={a}s label annotations task using PLS method (A) and the proposed segmentation method (B). Results are shown for different classifiers (Tree, KNN, NB, LR, SVM) and local (L), contextual (C) and local together with contextual (L+C) features. DTW is the baseline method used for comparison. The best random baseline F-score is  0.153 obtained using RB2. } 
	\label{tab:accuraciesRegion}
	%	\vspace{-1.5em}
\end{table}


In general, we see that the proposed segmentation method improves the performance over PLS method in both tasks, wherein the differences are statistically significant in the former case. Furthermore, the local feature set, when combined with the proposed segmentation method, yields the best accuracies. We also find that the contextual features do not complement the local features to further improve the performance. However, interestingly, they perform reasonably good considering that they only use contextual information.


\subsection{Conclusions and Future work}
\label{ConclusionAndFutureWork}

We proposed a method for detecting ny\={a}s segments in melodies of Hindustani music. We divided the task into two broad steps: melody segmentation and segment classification. For melody segmentation we proposed a method which incorporates domain knowledge to facilitate ny\={a}s boundary annotations. We evaluated three feature sets: local, contextual and the combination of both. We showed that the performance of the proposed method is significantly better compared to a baseline method using standard dynamic time warping based distance and a $K$ nearest neighbor classifier. Furthermore, we showed that the proposed segmentation method outperforms a standard approach based on piece-wise linear segmentation. A feature set that includes only the local features was found to perform best. However, we showed that using just the contextual information we could also achieve a reasonable accuracy. This indicates that ny\={a}s segments have a defined melodic context which can be learned automatically. In the future we plan to perform this task on Bandish performances, which is a compositional form in Hindustani music. We also plan to investigate other melodic landmarks and different evaluation measures for label annotations.





