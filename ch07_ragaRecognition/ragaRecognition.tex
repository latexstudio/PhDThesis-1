%!TEX root = ../thesis_a4.tex

\chapter{\titlecap{Automatic \glsentrytext{raga} Recognition}}
\label{chap:raga_recognition}


\section{Introduction}
\label{sec:raga_rec_intro}

In this chapter, we address the task of automatically recognizing \glspl{raga} in audio recordings of \gls{iam}. We describe two novel approaches for \gls{raga} recognition that jointly capture the tonal and the temporal aspects of melody. The contents of this chapter are largely based on our published work in~\cite{gulati_tdms_2016,gulatiphrase_2016}.

\Gls{raga} is a core musical concept used in compositions, performances, music organization, and pedagogy of \gls{iam}. Even beyond the art music, numerous compositions in Indian folk and film music are also based on \glspl{raga}~\citep{ganti2013bollywood}. \Gls{raga} is therefore one of the most desired melodic descriptions of a recorded performance of \gls{iam}, and an important criterion used by listeners to browse its audio music collections. Despite its significance, there exists a large volume of audio content whose \gls{raga} is incorrectly labeled or, simply, unlabeled. A computational approach to \gls{raga} recognition will allow us to automatically annotate large collections of audio music recordings. It will enable \gls{raga}-based music retrieval in large audio archives, semantically-meaningful music discovery and musicologically-informed navigation. Furthermore, a deeper understanding of the \gls{raga} framework from a computational perspective will pave the way for building applications for music pedagogy in \gls{iam}. 

\Gls{raga} recognition is the most studied research topic in \gls{mir} of \gls{iam}. There exist a considerable number of approaches utilizing different characteristic aspects of \glspl{raga} such as \gls{svara} set, \gls{svara} salience and \gls{arohana}-\gls{avrohana}. A critical in-depth review of the existing approaches for \gls{raga} recognition is presented in \secref{sec:sota_raga_recognition}, wherein we identify several shortcomings in these approaches and possible avenues for scientific contribution to take this task to the next level. Here we provide a short summary of this analysis: 

\begin{itemize}
	\item Nearly half of the number of the existing approaches for \gls{raga} recognition do not utilize the temporal aspects of melody at all (\tabref{tab:raga_recognition_methods_melodic_characteristics}), which are crucial in characterizing \glspl{raga}. Approaches that do utilize the temporal aspects, invariably consider a discrete representation of melody in the analysis. Thus, they do not capture the characteristics of the continuous melodic transitions across the \glspl{svara}, which may be relevant for \gls{raga} recognition. Therefore, approaches that can work with a continuous melody representation and still can capture effectively the temporal aspects of melody are worth exploring for this task.
	
	\item The musical complexity in recognizing \glspl{raga} depends on both the number and the specific set of \glspl{raga} to be distinguished. A large number of the existing approaches for this task are evaluated using different datasets, which typically comprise a small number of \glspl{raga} with only a handful of recordings per \gls{raga}. Therefore, a reliable assessment and comparison of the existing approaches becomes a challenging task. Thus, a sizable representative dataset of Hindustani and Carnatic music that can be openly shared and used by the community will be instrumental in systematically improving the state of the art in \gls{raga} recognition.
	
	\item A comparative evaluation of the existing methods is missing in the literature. Most of the studies only evaluate their proposed method and do not perform any comparison with the existing studies. A comprehensive comparison of different approaches on the same dataset and under the same experimental setup is needed in order to identifying the strengths and weaknesses of these methods.
	
\end{itemize}

%\begin{figure}
%	\begin{center}
%		\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/bd_overall_raga_recognition.pdf}
%	\end{center}
%	\caption{Overall block diagram for automatic \gls{raga} recognition.}
%	\label{fig:bd_raga_recognition}
%\end{figure}

In this section, we describe two novel methods for \gls{raga} recognition \acrshort{ragarecVSM} and \acrshort{ragarecTDMS} that overcome a number of shortcomings in the existing approaches as enumerated above and in our literature review (\secref{sec:sota_raga_recognition}). In addition, we compile and curate sizable datasets of Hindustani and Carnatic music for evaluating our methods. To the best of our knowledge these are the largest datasets every used for this task. We also make these datasets publicly available online (\appref{app:resources}). We now proceed to describe our methods in detail. 

%A general block diagram of \acrshort{ragarecVSM} and \acrshort{ragarecTDMS} is shown in Figure~\ref{fig:bd_raga_recognition}. There are two main processing blocks: feature extraction and classification. Given sets of audio recordings for training ($\mathrm{\corpus}_\mathrm{tr}$) and testing ($\mathrm{\corpus}_\mathrm{ts}$) we first extract features ($\mathrm{F}_\mathrm{tr}$ and $\mathrm{F}_\mathrm{ts}$) for every audio recording in both the sets. Subsequently features of the training set $\mathrm{F}_\mathrm{tr}$ along with the \gls{raga} labels $\mathrm{L}_\mathrm{tr}$ are used to train a classifier to build a classification model. Using this model we then predict the \gls{raga} labels for the testing set $\mathrm{L}_\mathrm{ts}$. \acrshort{ragarecVSM} and \acrshort{ragarecTDMS} differ both in terms of the extracted features and the classification strategy. Both these methods are described at length in the subsequent sections.



\section{\titlecap{Pattern-based \glsentrytext{raga} Recognition}}
\label{sec:pattern_based_raga_recognition}

In this section, we describe our pattern-based approach (\acrshort{ragarecVSM}) to \gls{raga} recognition. \acrshort{ragarecVSM} utilizes melodic patterns automatically discovered from audio recordings using our approach described in \chapref{chap:melodic_pattern_processing}, and employs the vector space modeling concept to build a \gls{raga} model from these melodic patterns. This section is based on our published work presented in~\cite{gulatiphrase_2016}.

As mentioned before, every \gls{raga} has a set of characteristic melodic patterns that capture the essence of the \gls{raga}. These melodic patterns are one of the most prominent cues for \gls{raga} identification, used by a performer, as well as by a listener (\secref{sec:melody_in_iam}). They act as a building block to construct melodies both in musical compositions and improvisations. However, despite the importance of melodic patterns in characterizing \glspl{raga}, they have not been fully exploited by the computational methods for \gls{raga} recognition. There exist only a handful of methods that utilize melodic patterns for this task (\secref{sec:sota_raga_recognition}). These methods work with a discrete representation of melody and pre-defined dictionaries of melodic patterns, which severely limits the potential of a pattern-based approach for \gls{raga} recognition. To the best of our knowledge, there exists only one method that uses automatically discovered melodic patterns for this task~\citep{shrey_ISMIR_2015}. However, the melodic patterns are extracted from specific short duration regions (\gls{pallavi} lines) of recordings in Carnatic music, and therefore, the scalability of this method on large audio collections is questionable. Furthermore, the authors address the task of \gls{raga} verification, which is less challenging compared to \gls{raga} recognition. A detailed discussion on the existing methods and their shortcomings is provided in \secref{sec:sota_raga_recognition}.

Before formally describing our method, we first present the intuition and motivation behind the approach. A number of similarities can be seen between a \gls{raga} rendition and a textual description of a topic. Like an author describes a topic by using different words relevant to the topic, an artist renders a \gls{raga} by using appropriate melodic phrases that suit the context. There are words that are quite specific to a topic, which are analogous to the characteristic melodic phrases of a \gls{raga}. Stop words, which are not specific to any topic or to a document can be seen as generic \gls{gamaka} type melodic patterns, which are not specific to a \gls{raga} or to a recording (\secref{sec:recurring_melodic_patterns_iam}). Words that are specific to a document are analogous to composition specific patterns. This analogy drives our method and motivates us to employ concepts of \gls{vsm} to perform \gls{raga} recognition using melodic patterns. We now proceed to describe \acrshort{ragarecVSM} in detail.

 %As shown in~\figref{fig:bd_raga_recognition} there are two main processing blocks: feature extraction and classification. In the first block we extract TFIDF-based features, wherein we consider automatically discovered melodic patterns as different `terms' (\secref{sec:vsm_feature_extraction}). Based on these features, in the second block we train a classifier to build a model for each \gls{raga}. 

%\COMMENT{if time permits write pros and cons of a phrase based approach?, PROS: can enable real-time raga rec}

%Merits and shortcomings of this method are summarized below:
%\begin{itemize}
%	\item Human interpretation of the intermediate output, usability of intermediate outputs, exploiting the most characteristic aspect of a raga. Utilize both the tonal and the pitch information. Exploit characteristic movements between the svaras as well as sequence of svaras
%	\item No need for analyzing whole melody, if patterns are identified in chunks raga can be identified.
%	\item Musically meaningful relatinos between recordings can be established.
%\end{itemize}
%
%Cons
%\begin{itemize}
%	\item Prone to octave errors, not statistical, spurts of error can deprove the performance enormously
%	\item Extraction of characteristic melodic patterns is still a challenging process. Limited by that performance
%	\item Affected a lot by the improvisational aspects
%	\item Computationally challenging. 
%	\item Does not exploit global melodic characteristics.
%\end{itemize}
%




\subsection{Vector Space Modeling of Melodic Patterns}
\label{sec:vsm_feature_extraction}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeSeventy]{ch07_ragaRecognition/figures/bd_phasebased_raga_recognition.pdf}
	\end{center}
	\caption{Block diagram of the proposed phrase-based approach to \gls{raga} recognition.}
	\label{fig:bd_phasebased_raga_recognition}
\end{figure}

The block diagram of \acrshort{ragarecVSM} is shown in Figure~\ref{fig:bd_phasebased_raga_recognition}. There are three main processing blocks: melodic pattern discovery, pattern clustering and \gls{tfidf}-based feature extraction. We describe each of these blocks in the following sections. 

\subsubsection{Melodic Pattern Discovery}
\label{sec:vsm_feature_extraction_pattern_discovery}

In this block, we extract repeating melodic patterns from the collection of audio recordings. For this, we employ the pattern discovery method described in~\secref{sec:patterns_melodic_pattern_discovery}. There are three main processing modules in our pattern discovery method: data-processing, intra-recording pattern discovery and inter-recording pattern search, which are already described at length in~\secref{sec:patterns_discovery_method}. 

The results of the experiments presented in this section are provided for the following parameter settings used in the pattern discovery block. These parameter values are set based on our learnings in different experiments described in~\chapref{chap:melodic_pattern_processing}. We use the predominant pitch sampled at 45\,Hz for Hindustani music and 55\,Hz for Carnatic music. We apply all the post-processing procedures described in~\secref{sec:data_preprocessing_pitch_postprocessing} with the same set of parameter values. We extract melodic patterns of length 2\,s in both the music traditions. In addition to normalizing the predominant pitch by the tonic of the recording, we also perform an octave normalization while comparing patterns~(\secref{sec:patterns_improving_similarity_transposition_invariance}). We extract 30\,closest pattern pairs within each recording, and for each of them their 20\,closest patterns across the recordings. We use a global \gls{dtw} band constraint with 10\%~bandwidth. The local cost in \gls{dtw} distance is computed using the squared Euclidean function. For each melodic pattern candidate its five time-scaled versions are considered in both intra and inter-recording pattern search as described in~\secref{sec:patterns_melodic_similarity_time_scaling}. We also perform duration truncation of the steady \glspl{svara} in the melodies~(\secref{sec:patterns_improving_similarity_svara_duration_trucation}). We truncate \glspl{svara} to a maximum of 500\,ms and 300\,ms for Hindustani and Carnatic music melodies, respectively. These values are derived from our results presented in~\secref{sec:patterns_improving_similarity_results_and_discussion}. More detailed information about the system parameters and configurations can be obtained from the companion web page of the relevant article~\citep{gulatiphrase_2016}\footnote{\url{http://compmusic.upf.edu/node/278}}.

Note that the output of the pattern discovery method contains different types of repeated melodic patterns with varied degree of their musical relevance (\secref{sec:patterns_characterization_of_melodic_patterns}). In this step we do not filter any melodic pattern based on their relevance with respect to \glspl{raga}. A soft selection of the relevant patterns is implicitly done in our methodology described in the subsequent sections.


\subsubsection{Melodic Pattern Clustering}
\label{sec:vsm_feature_extraction_pattern_clustering}

In order to effectively utilize the discovered melodic patterns for \gls{raga} recognition, it is important to cluster together all the patterns that are different occurrences of the same underlying melodic phrase. For this, we propose to perform a network analysis, wherein the clustering is performed using a non-overlapping community detection method. The network analysis and clustering process used here is the same as described in~\secref{pattern_characterization}, but with a different end goal of characterizing melodic patterns. For the sake of completeness we here provide a brief description of the process we follow.

We start by building an undirected network $\netUndirWght$ using the discovered patterns as the nodes of the network. We connect any two nodes only if the distance between them is below a similarity threshold $\simThsld$. Noticeably, the distance between two melodic patterns is computed using the same measure as used in the intra-recording pattern discovery block~(\secref{sec:intraRecordingPatternDiscovery}). The weight of the edge, when it exists, is set to 1, and all non-connected nodes are removed from the network.



As discussed in~\secref{sec:patterns_characterization_of_melodic_patterns} determining a meaningful similarity threshold in an unsupervised manner is a challenging task. For estimating an optimal value of $\simThsld$ we follow the approach as described in~\secref{sec:network_filtering}. We compare the evolution of the clustering coefficient $\clusCoff$ of the obtained network $\netUndirWght$ with the clustering coefficient of a randomized network $\netUndirWght_r$ over different distance thresholds $\simThsld$. The randomized network $\netUndirWght_r$ is obtained by swapping the edges between randomly selected pairs of nodes such that the degree of each node is preserved~\citep{maslov2002specificity}. The optimal threshold $\simThsld^\star$ is taken as the distance that maximizes the difference between the two clustering coefficients. In~\figref{fig:raga_rec_clustering_coff_evolution}, we show $\clusCoff(\netUndirWght)$, $\clusCoff(\netUndirWght_r)$ and $\clusCoff(\netUndirWght)-\clusCoff(\netUndirWght_r)$ for different values of $\simThsld$, and mark the optimal threshold $\simThsld^\star$.

\begin{figure}
	\begin{center}
		\ifdefined\PRINTVER		
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/ClusteringCoffs_40raga_2s_BW.pdf}
		\else
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/ClusteringCoffs_40raga_2s.pdf}
		\fi
	\end{center}
	\caption[Evolution of $\clusCoff(\netUndirWght)$, $\clusCoff(\netUndirWght_r)$ and $\clusCoff(\netUndirWght)$ - $\clusCoff(\netUndirWght_r)$ over values of $\simThsld$]{Evolution of clustering coefficients of $\netUndirWght$ and $\netUndirWght_r$ and their difference for different similarity thresholds~($\simThsld$).}
	\label{fig:raga_rec_clustering_coff_evolution}
\end{figure}

In the next step of our method we group together similar melodic patterns~(\figref{fig:bd_phasebased_raga_recognition}). To do so, we detect non-overlapping communities in the network of melodic patterns using the method proposed by~\cite{blondel2008fast}. This community detection method is based on optimizing the modularity of the network and is parameter-free from the user's point of view. This method is capable of handling very large networks and has been extensively used in various applications~\citep{fortunato2010community}. We use its implementation available in networkX~\citep{hagberg-2008-exploring}, a Python language package for exploration and analysis of networks and network algorithms. Note that, from now on, the melodic patterns grouped within a community are regarded as the occurrences of a single melodic phrase. Thus, a community essentially represents a melodic phrase or motif.

\subsubsection{TFIDF Feature Extraction}
\label{sec:vsm_feature_extraction_TFID_computation}

As mentioned above, we draw an analogy between \gls{raga} rendition and textual description of a topic. Using this analogy we represent each audio recording using a vector space model, wherein melodic patterns are considered as words (or terms). This process is divided into three blocks (Figure~\ref{fig:bd_phasebased_raga_recognition}).

We start by building our vocabulary $\pattVocab$, which translates to selecting relevant pattern communities for characterizing \glspl{raga}. For this, we include all the detected communities except the ones that comprise patterns extracted from only a single audio recording. Such communities are analogous to the words that only occur within a document and, hence, are irrelevant for modeling a topic.%The size of the obtained vocabulary $\pattVocab$ corresponding to the optimal threshold mentioned above ($\simThsld^\star=9$) is XXX.

We experiment with three different sets of features $\featureVSMOne$, $\featureVSMTwo$ and $\featureVSMThree$, which are similar to the \gls{tfidf} features typically used in text information retrieval. We denote our corpus by $\corpus$ comprising $\nRecCorpus = |\corpus|$ number of recordings. A melodic phrase and a recording is denoted by $\pattern$ and $\recording$ , respectively

\begin{equation}
\featureVSMOne(\pattern,\recording)= 
\begin{cases}
1				,& \text{if}~~\feaqPhRec(\pattern,\recording) > 0\\
0,              & \text{otherwise}
\end{cases}
\end{equation}
where, $\feaqPhRec(\pattern,\recording)$ denotes the raw frequency of occurrence of pattern $\pattern$ in recording $\recording$. $\featureVSMOne$ only considers the presence or absence of a pattern in a recording. In order to investigate if the frequency of occurrence of melodic patterns is relevant for characterizing \glspl{raga}, we take $\featureVSMTwo(\pattern,\recording) = \feaqPhRec(\pattern,\recording)$. As mentioned, the melodic patterns that occur across different \glspl{raga} and in several recordings do not aid \gls{raga} recognition. Therefore, to reduce their effect in the feature vector we employ a weighting scheme, similar to the inverse document frequency ($\mathrm{idf}$) weighting in text retrieval.
\begin{equation}
\featureVSMThree(\pattern,\recording) = \feaqPhRec(\pattern,\recording)  \irf(\pattern,\corpus)
\end{equation}
\begin{equation}
\irf(\pattern,\corpus) = \log \left( \frac{\nRecCorpus}{ |\lbrace \recording \in \corpus: \pattern \in \recording \rbrace|} \right)
\end{equation}
where, $|\lbrace \recording \in \corpus: \pattern \in \recording \rbrace|$ is the number of recordings where the melodic pattern $\pattern$ is present, that is $\feaqPhRec(\pattern,\recording)\neq 0$ for these recordings. 

\subsection{Evaluation}
\label{sec:raga_rec_pattern_evaluation}

\subsubsection{Music Collection}
\label{sec:raga_rec_pattern_music_collection}

The music collection used for evaluation in this study is a subset of the CompMusic Carnatic music corpus (\secref{sec:corpus_carnatic_music_corpus}). Due to the differences in the melodic characteristics of Carnatic and Hindustani music, to evaluate our method we compile and curate two datasets, \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}, one for each music tradition. A separate evaluation on both the music traditions allows a better analysis and interpretation of the results.

\acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} comprise 124 and 130~hours of commercially available audio recordings, respectively. All the editorial metadata for each audio recording is publicly available in MusicBrainz\footnote{https://musicbrainz.org/}, an open-source metadata repository. \acrshort{rrds_cmd_big} contains full-length recordings of 480~performances belonging to 40~\glspl{raga} with 12~music pieces per \gls{raga}. \acrshort{rrds_hmd_big} contains full-length recordings of 300~performances belonging to 30~\glspl{raga} with 10~music pieces per \gls{raga}. The selected music material is diverse in terms of the number of artists, the number of forms, and the number of compositions, and thus, it is representative of these music traditions. The chosen \glspl{raga} contain diverse sets of \glspl{svara}, both in terms of the number of \glspl{svara} and their pitch-classes (\glspl{svarsthana}).

To the best of our knowledge, these are the largest and the most comprehensive (in terms of the available metadata) datasets ever used for studying the task of automatic \gls{raga} recognition. To facilitate reproducible research and comparative studies we also make these datasets publicly available online (\appref{app:resources}). A further detailed description of these datasets is provided in~\secref{sec:corpus_raga_recognition_datasets}. 

\subsubsection{Classification and Experimental Setup}
\label{sec:raga_rec_pattern_classification_evaluation}

The features obtained above are used to train a classifier. In order to assess the relevance of these features for \gls{raga} recognition, we experiment with different algorithms exploiting diverse classification strategies~\citep{Hastie09BOOK}: \acrfull{nbm}, \acrfull{nbg}, and \acrfull{nbb}, support vector machines with a linear and a radial basis function kernel, and with a stochastic gradient descent learning (\acrshort{svml}, \acrshort{svmr} and \acrshort{sgd}, respectively), \acrfull{lr} and \acrfull{randforest}. We use the implementation of these classifiers available in scikit-learn toolkit~\citep{scikitlearn}, version 0.15.1. Since in this study, our focus is to extract a musically relevant set of features based on melodic patterns, we use the default parameter settings for the classifiers available in scikit-learn. 

We use leave-one-out cross validation methodology for evaluations~\citep{Mitchell97BOOK}, in which one recording in the evaluation dataset forms the testing set and the remaining ones become the training set. We use the mean classification accuracy across recordings as the evaluation measure. To assess if the difference in the performance between any two methods is statistically significant, we use McNemar's test~\citep{mcnemar1947note} with $p < 0.01$. In addition, to compensate for multiple comparisons, we apply the Holm-Bonferroni method~\citep{holm1979simple}. 

Note that in our published work in~\cite{gulatiphrase_2016} we use a different evaluation strategy (12-fold cross-validation methodology). Therefore the results provided in the article differ slightly form the ones presented in this thesis. In addition, the method for assessing statistical significance is also different (Mann-Whitney U test), which is due to the difference in the evaluation strategy. The evaluation strategy used here (leave-one-out cross validation) does not involve any random sampling (or split) of the evaluation set, which makes our experimental setup more definite.

\subsubsection{Comparison with the State of the art}
\label{sec:raga_rec_pattern_comparison_sota}

We compare our results with two state of the art methods proposed in~\citep{chordia2013joint} and \citep{koduri2014intonation}. As an input to these methods, we use the same features, predominant pitch and tonic, as used in our method. The only difference is that the pitch contours fed to these methods are not post-processed (\secref{sec:data_preprocessing_pitch_postprocessing}). This is because these methods primarily exploit the intonation aspect of the \glspl{svara}, and therefore, performing a smoothening operation on pitch contours might degrade their performance. The method in~\cite{chordia2013joint} uses smoothened \acrfull{pcd} as the tonal feature and employs \acrfull{1nn} using Bhattacharyya distance for predicting \gls{raga} labels. We denote this method by \acrshort{sotaChordia}. The authors in~\cite{chordia2013joint} report a window size of 120\,s as an optimal duration for computing \glspl{pcd} (denoted here by $\mathrm{PCD}_{120}$). However, in our experiments we find that \glspl{pcd} computed over the entire audio recording (denoted here by~$\mathrm{PCD}_\mathrm{full}$) result in a significant improvement~\citep{gulatiphrase_2016}. We therefore use $\mathrm{PCD}_\mathrm{full}$ for comparison. Note that in~\cite{chordia2013joint} the authors do not experiment with a window size larger than 120\,s. 

The method in~\cite{koduri2014intonation} is proposed primarily for Carnatic music. This method also uses features based on pitch distribution. However, unlike in \acrshort{sotaChordia}, the authors use parameterized pitch distributions as features. We denote this method by \acrshort{sotaKoduri}. We consider two variants of this method. One in which the parameterization is done using a single pitch histogram that includes distribution of all the \glspl{svara} in the recording (denoted here by $\mathrm{PD}_\mathrm{param}$), and the other, wherein a separate histogram is constructed for each individual \gls{svara} in the recording (denoted here by $\mathrm{PD}_\mathrm{context}$). In the second variant the mapping of a pitch sample to a \gls{svara} is based on its local melodic context as explained in~\cite{koduri2014intonation}. Note that \acrshort{sotaKoduri} utilizes specific intonation aspects of \glspl{svara} in Carnatic music, and is not devised and tested for Hindustani music recordings. We therefore do not consider this method for comparing results on Hindustani music. 

The authors of \acrshort{sotaChordia} courteously ran the experiments on our dataset using the original implementations of the method. For \acrshort{sotaKoduri}, the authors kindly extracted the features ($\mathrm{PD}_\mathrm{param}$ and $\mathrm{PD}_\mathrm{context}$) using the original implementation of their method and the experiments using different classification strategies were done by us.


\subsection{Results and Discussion}
\label{sec:vsm_eval_results}

Before we proceed to present our results, we notify readers that the accuracies reported in this section for different methods vary slightly from the ones reported in~\cite{gulatiphrase_2016}. As mentioned, the experimental setup used here is different compared to the paper (\secref{sec:raga_rec_pattern_classification_evaluation}). In addition, the parameters used for discovering melodic patterns are also different~(\secref{sec:vsm_feature_extraction_pattern_discovery}). Furthermore, we do not consider the dataset that comprise only 10\,\gls{raga} as done in~\cite{gulatiphrase_2016}, for which our method was shown to outperform the state of the art. 

\begin{table}
	\centering
	\renewcommand{\arraystretch}{1.5}	
\begin{tabular}{ c|c|c c c c c c c }
\tabletop
	Method & Feature & \acrshort{svml} & \acrshort{sgd} & \acrshort{nbm} & \acrshort{nbg} & \acrshort{randforest} & \acrshort{lr} & \acrshort{1nn}\tabularnewline
\tablemid
	\multirow{3}{*}{\acrshort{ragarecVSM}} & $\featureVSMOne$ & 51.04 & 55 & 37.5 & 54.37 & 25.41 & 55.83 & -\tabularnewline

	& $\featureVSMTwo$ & 45.83 & 50.41 & 35.62 & 47.5 & 26.87 & 51.87 & -\tabularnewline

	& $\featureVSMThree$ & 45.83 & 51.66 & \textbf{67.29} & 44.79 & 23.75 & 51.87 & -\tabularnewline
\tablemid
	\acrshort{sotaChordia} & $\mathrm{PCD}_\mathrm{full}$ & - & - & - & - & - & - & \textbf{73.12}\tabularnewline
\tablemid
	\multirow{2}{*}{\acrshort{sotaKoduri}} & $\mathrm{PD}_\mathrm{param}$ & 30.41 & 22.29 & 27.29 & 28.12 & 42.91 & 30.83 & 25.62\tabularnewline

	 & $\mathrm{PD}_\mathrm{context}$ & 54.16 & 43.75 & 5.2 & 33.12 & 49.37 & \textbf{54.79} &26.25 \tabularnewline
\tablebot
\end{tabular}	
	\caption[Accuracy of \acrshort{ragarecVSM}, \acrshort{sotaChordia} and \acrshort{sotaKoduri} on \acrshort{rrds_cmd_big}]{Accuracy (\%) of \gls{raga} recognition on \acrshort{rrds_cmd_big} dataset by \acrshort{ragarecVSM} and other methods methods using different features and classifiers. Bold text signifies the best accuracy by a method among all its variants} 
	\label{tab:accuracies_cmd_vsm}
\end{table}



\begin{table}
	\centering
	\renewcommand{\arraystretch}{1.5}	
\begin{tabular}{c|c|ccccccc}
\tabletop
	Method & Feature & \acrshort{svml} & \acrshort{sgd} & \acrshort{nbm} & \acrshort{nbg} & \acrshort{randforest} & \acrshort{lr} & \acrshort{1nn}\tabularnewline
\tablemid
	\multirow{3}{*}{\acrshort{ragarecVSM}} & $\featureVSMOne$ & 71 & 72.33 & 69.33 & 79.33 & 38.66 & 74.33 & -\tabularnewline
	& $\featureVSMTwo$ & 65.33 & 64.33 & 67.66 & 72.66 & 40.33 & 68 & -\tabularnewline
	& $\featureVSMThree$ & 65.33 & 62.66 & \textbf{82.66} & 72 & 41.33 & 67.66 & -\tabularnewline
	\hline 
	\acrshort{sotaChordia} & $\mathrm{PCD}_\mathrm{full}$ & - & - & - & - & - & - & \textbf{91.66}\tabularnewline
\tablebot
\end{tabular}
	\caption[Accuracy of \acrshort{ragarecVSM} and \acrshort{sotaChordia} on \acrshort{rrds_hmd_big}]{Accuracy (\%) of \gls{raga} recognition on \acrshort{rrds_hmd_big} dataset by \acrshort{ragarecVSM} and other methods methods using different features and classifiers. Bold text signifies the best accuracy by a method among all its variants.} 
	\label{tab:accuracies_hmd_vsm}
\end{table}

In~\tabref{tab:accuracies_cmd_vsm} and~\tabref{tab:accuracies_hmd_vsm}, we present the results of our proposed method \acrshort{ragarecVSM} and the two state of the art methods \acrshort{sotaChordia} and \acrshort{sotaKoduri} for the two datasets \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}, respectively. The highest accuracy for every method is highlighted in bold for both the datasets. Due to the poor performance of \acrshort{svml} and \acrshort{nbb} classifiers in the experiments we do not consider them in our further analysis. 

We start by analyzing the results of the variants of \acrshort{ragarecVSM} for both the datasets. From~\tabref{tab:accuracies_cmd_vsm} and~\tabref{tab:accuracies_hmd_vsm}, we see that the highest accuracy obtained by \acrshort{ragarecVSM} for \acrshort{rrds_cmd_big} is 67.29\%, and for \acrshort{rrds_hmd_big} is 82.66\%. The difference in the performance across the two datasets can be attributed to several factors. One of them being the difference in the number of \glspl{raga} across the two datasets: 40 in \acrshort{rrds_cmd_big} and 30 \acrshort{rrds_hmd_big}. The performance difference can also be because of the difference in the overall length of the audio recordings. Recordings of the music pieces considered in our datasets are significantly longer in Hindustani music compared to Carnatic music (\secref{sec:corpus_raga_recognition_datasets}). However, since the melodic characteristics and complexity varies considerably across the two music traditions, a definite reason can only be identified by performing the experiments with equal number of \glspl{raga} in the dataset and equal duration of the music pieces (\secref{sec:ragarec_dataset_effect}). 

From~\tabref{tab:accuracies_cmd_vsm} and~\tabref{tab:accuracies_hmd_vsm}, we see that for both the datasets, the accuracy obtained by \acrshort{ragarecVSM} across the feature sets differs substantially. We also see that their performance is very sensitive to the choice of the classifier. With the exceptions of the \acrshort{nbm} and \acrshort{lr} classifier, feature $\featureVSMOne$ in general performs better than the other two features and the difference is found to be statistically significant in each case. This suggests that, considering just the presence or the absence of a melodic pattern, irrespective of its frequency of occurrence, might be sufficient for \gls{raga} recognition. Interestingly, this finding is consistent with the fact that characteristic melodic patterns are unique to a \gls{raga} and a single occurrence of such patterns is sufficient to identify the \gls{raga}~\citep{krishna2012carnatic}. However, the best performance is obtained by the combination of $\featureVSMThree$ and \acrshort{nbm} classifier, and the difference in its performance compared to any other variant is statistically significant. The \acrshort{nbm} classifier outperforming other classifiers using appropriate features is also well recognized in the text classification community~\citep{mccallum1998comparison}. We, therefore, only consider the combination of $\featureVSMThree$ and the \acrshort{nbm} classifier for comparing \acrshort{ragarecVSM} with the other methods. Overall, the accuracies across different features indicate that normalizing the frequency of occurrence of melodic patterns (as done for $\featureVSMOne$ and $\featureVSMThree$) is beneficial for \gls{raga} recognition. A plausible reason can be that a normalization procedure reduces the weight of the type of melodic patterns that occur more frequently, such as the \glspl{gamaka} type patterns~(\secref{sec:melody_in_iam}), and are not the characteristic patterns of any particular \gls{raga}. It is worth noting that the feature weights assigned by a classifier can be used to identify the relevant melodic patterns for \gls{raga} recognition. These patterns can serve as a dictionary of semantically-meaningful melodic units for many computational tasks in \gls{iam}.

\begin{figure}

	\begin{subfigure}{\textwidth}
			\centering
		\ifdefined\PRINTVER	
			\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/CarnaticConfig3_Accuracy_vs_CCdiff_BW.pdf}
		\else
			\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/CarnaticConfig3_Accuracy_vs_CCdiff.pdf}
		\fi
		\caption{For \acrshort{rrds_cmd_big} dataset}
		\label{fig:performance_across_thresholds_cmd}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
			\centering
		\ifdefined\PRINTVER
			\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/HindustaniConfig2_Accuracy_vs_CCdiff_BW.pdf}
		\else
			\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/HindustaniConfig2_Accuracy_vs_CCdiff.pdf}
		\fi
		\caption{For \acrshort{rrds_hmd_big} dataset}
		\label{fig:performance_across_thresholds_hmd}
	\end{subfigure}
		\caption[Accuracy of \acrshort{ragarecVSM} and $\clusCoff(\netUndirWght)-\clusCoff(\netUndirWght_r)$ for different values of $\simThsld$]{Accuracy of \acrshort{ragarecVSM} and $\clusCoff(\netUndirWght)-\clusCoff(\netUndirWght_r)$ for different similarity thresholds ($\simThsld$) for both the datasets.}
		\label{fig:performance_across_thresholds}
\end{figure}


In addition to analyzing the final \gls{raga} recognition accuracies, we also verify our approach to obtain the optimal similarity threshold $\simThsld^\star$~(\secref{sec:vsm_feature_extraction_pattern_clustering}). For this, we perform \gls{raga} recognition using different similarity thresholds $\simThsld$. In~\figref{fig:performance_across_thresholds}, we show the accuracy obtained by \acrshort{ragarecVSM}, and $C(\netUndirWght)-C(\netUndirWght_r)$ as a function of similarity threshold $\simThsld$ for both the datasets. We see that these curves are highly correlated for both the datasets. Thus, we see that our strategy to obtain the optimal threshold $\simThsld^\star$, which we defined in~\secref{sec:vsm_feature_extraction_pattern_clustering} as the distance ($\simThsld$) that maximizes the difference $C(\netUndirWght)-C(\netUndirWght_r)$, is successful and, it results in the best \gls{raga} recognition accuracy.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\figSizeNinety]{ch07_ragaRecognition/figures/CM_vsm_cmd_var1.pdf}
	\end{center}
	\caption[Confusion matrix of the classification results by \acrshort{ragarecVSM} on \acrshort{rrds_cmd_big}]{Confusion matrix of the \gls{raga} predictions by \acrshort{ragarecVSM} on \acrshort{rrds_cmd_big} dataset. The different shades of grey are mapped to different number of audio recordings.}
	\label{fig:confusion_matrix_cmd}
\end{figure}

We now analyze the confusion matrix to understand the type of classification errors made by \acrshort{ragarecVSM}. In~\figref{fig:confusion_matrix_cmd} we show the confusion matrix of the \gls{raga} predictions on \acrshort{rrds_cmd_big} dataset. We observe that our method achieves near-perfect accuracy for several \glspl{raga} including \gls{ritigaula}, \gls{behag}, \gls{mukhari}, \gls{mayamalavagaula}, \gls{sankarabharanam}, \gls{nata}, \gls{todi}, \gls{kalyani}, \gls{purvikalyani} and \gls{varali}. This is consistent with the fact that these are considered to be phrase-based \glspl{raga}, that is, their identity is predominantly derived from melodic phraseology~\citep{krishna2012carnatic}. From~\figref{fig:confusion_matrix_cmd} we notice that the frequent confusions are between \gls{raga} \gls{sindhubhairavi} and \gls{todi}, \gls{saveri} and \gls{mayamalavagaula}, \gls{sri} and \gls{madhyamavati}, \gls{atana} and \gls{madhyamavati}, \gls{sriranjani} and \gls{karaharapriya} and \gls{saama} and \gls{sahana}. On investigating it further we find that in almost every case of confusion the pair comprises allied \glspl{raga} (\secref{sec:allied_ragas}), i.e.~\glspl{raga} that have a common set of \glspl{svara} and similar melodic movements. Distinguishing between allied \glspl{raga} is a challenging task, since it is based on subtle melodic nuances. We also note that, some cases where the accuracy is relatively low correspond to scale-based \glspl{raga}. This is in line with~\cite{krishna2012carnatic}, where the authors remark that the identification of such \glspl{raga} is not based on melodic phraseology.
 
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\figSizeNinety]{ch07_ragaRecognition/figures/CM_vsm_hmd_var1.pdf}
	\end{center}
	\caption[Confusion matrix of the classification results by \acrshort{ragarecVSM} on \acrshort{rrds_hmd_big}]{Confusion matrix of the \gls{raga} predictions by \acrshort{ragarecVSM} on \acrshort{rrds_hmd_big} dataset. The different shades of grey are mapped to different number of audio recordings.}
	\label{fig:confusion_matrix_hmd}
\end{figure}

We now analyze the confusion matrix of the \gls{raga} predictions on \acrshort{rrds_hmd_big} dataset (\figref{fig:confusion_matrix_hmd}). We observe that our method achieves good accuracy for \gls{raga} \gls{miyan_malhar}, \gls{suddh_sarang}, \gls{alahaiya_bilaval}, \gls{sri}, \gls{lalit}, \gls{yaman_kalyan}, \gls{kedar}, \gls{gaud_malhar} and \gls{bilasakhani_todi}. Similar to the case of Carnatic music, these \glspl{raga} are phrase-based \glspl{raga} of Hindustani music. For common confusions as well we find the same reason that most of them occur between allied \glspl{raga}. For example, we notice that \gls{ahira_bhairav} is confused with \gls{bhairav}. This is explicable since the former is derived from the latter \gls{raga} and they both share the same lower tetrachord structure. We see that \gls{raga} \gls{des} is confused with \gls{miyan_malhar}. Both these \glspl{raga} come within the malh\={a}r group of \glspl{raga} and bear commonalities. We find a similar explanation for most of the confusions in the predictions. Going ahead, we notice from~\figref{fig:confusion_matrix_hmd} that the performance of our method is poor for \glspl{raga} including \gls{abhogi}, \gls{jog} and \gls{madhukauns}. An interesting common factor across these \glspl{raga} is that they are pentatonic \glspl{raga} comprising five \glspl{svara} (\gls{jog} is pentatonic in ascending progression). This suggests that \glspl{raga} with less number of comprising \glspl{svara} are more prone to get confused with other \glspl{raga}. However, further instigation of this phenomenon is left for the future work. Overall, the analysis of the classification errors for both Carnatic and Hindustani music datasets suggests that the our proposed method is more suitable for recognizing the phrase-based \glspl{raga} compared to the scale-based \glspl{raga}. In addition, we see that the common mistakes done by the method are explicable from a musicological perspective. 



Finally, we compare \acrshort{ragarecVSM} with the state of the art methods \acrshort{sotaChordia} and \acrshort{sotaKoduri}. From~\tabref{tab:accuracies_cmd_vsm}, we see that \acrshort{ragarecVSM} outperforms \acrshort{sotaKoduri} and the difference is found to be statistically significant. When compared with \acrshort{sotaChordia}, the performance of \acrshort{ragarecVSM} is inferior on both the datasets, wherein it is worse on \acrshort{rrds_hmd_big} dataset. In both the cases the difference in the performance is statistically significant across the methods. One of the plausible reasons for this difference can be that \acrshort{sotaChordia} utilizes entire full length recordings for this task, whereas, \acrshort{ragarecVSM} uses only short-time melodic patterns which sum up to only a fraction of the total duration of the performances. Since the recordings are considerably longer in \acrshort{rrds_hmd_big} dataset, the difference in the accuracies is even more prominent. Note that for comparison in our study we use $\mathrm{PCD}_\mathrm{full}$ variant of \acrshort{sotaChordia}, which performs significantly better than the original variant proposed by the authors, $\mathrm{PCD}_{120}$~\citep{gulatiphrase_2016}. Interestingly, a further comparison of the results of \acrshort{ragarecVSM} and \acrshort{sotaChordia} for each \gls{raga} reveals that their performance is complementary. \acrshort{ragarecVSM} successfully recognizes several \glspl{raga} with high accuracy for which \acrshort{sotaChordia} performs poorly, and vice-versa. This becomes evident by comparing the confusion matrix of the \gls{raga} predictions by \acrshort{ragarecVSM}~(\figref{fig:confusion_matrix_cmd} and \figref{fig:confusion_matrix_hmd}) and \acrshort{sotaChordia}~(\figref{fig:confusion_matrix_cmd_chordia} and \figref{fig:confusion_matrix_hmd_chordia}). For example, for the case of \acrshort{rrds_cmd_big} dataset, \acrshort{ragarecVSM} performs better for \glspl{raga} \gls{madhyamavati}, \gls{mukhari} and \gls{harikambhoji} as compared to \acrshort{sotaChordia}. And, \acrshort{sotaChordia} performs better for \glspl{raga} \gls{kapi}, \gls{sindhubhairavi} and \gls{anandabhairavi} as compared to \acrshort{ragarecVSM}. This suggests that the proposed pattern-based method can be combined with the \gls{pcd}-based methods to achieve a higher \gls{raga} recognition accuracy.

Overall, our results indicate that the proposed phrase-based approach (\acrshort{ragarecVSM}) that uses melodic patterns discovered in an unsupervised setup is a successful strategy for \gls{raga} recognition. However, as we have seen in comparison with the other methods there is still scope for improvement. One of the advantages of this approach is the interpretability of the results. Since the classification is based on melodic phrases, the user can better understand why the classifier is assigning a certain \gls{raga} label. In addition, based on the weight that a classifier assigns to a particular feature, the user can also infer the importance of that pattern in characterizing the \gls{raga}. Furthermore, since this method does not require a pitch distribution of an entire music piece, and is solely based on melodic patterns, it can be used in a real-time \gls{raga} recognition setup. Finally, the results of \acrshort{ragarecVSM} also suggest that the discovered melodic patterns are musically relevant and can be used to perform higher level melodic analyses. 


\section{\titlecap{\glsentrylong{tdms} for \glsentrytext{raga} recognition}}
\label{sec:tdms_raga_recognition}

The method described in the previous section (\acrshort{ragarecVSM}) uses melodic patterns for recognizing \glspl{raga}. Using automatically discovered short-duration melodic patterns that constitute only a fraction of the total duration of the audio recordings, \acrshort{ragarecVSM} shows promising results by achieving an accuracy comparable to the state of the art method. As discussed before, \acrshort{ragarecVSM} has several advantages such as musically meaningful interpretation of the classification results. While we further refine our methodology for discovering melodic patterns and in turn improve \gls{raga} recognition, we also propose another approach (\acrshort{ragarecTDMS}) for this task.

Similar to \acrshort{ragarecVSM}, \acrshort{ragarecTDMS} aims to capture the tonal and the temporal characteristics of melody by using its continuous representation. However, instead of using short-duration patterns extracted directly from the surface representation of melody as done in \acrshort{ragarecVSM}, in \acrshort{ragarecTDMS} we seek to abstract the melody representation. An abstraction of melody that captures both the tonal and the temporal aspects relates to the concept of \gls{chalan} in \gls{iam} (\secref{sec:melody_in_iam}). \Gls{chalan} (literally meaning gait or movement) of a \gls{raga} defines its melodic outline in terms of how a melodic transition is to be made from one \gls{svara} to another, the precise intonation to be followed, and the proportion of time spent on each \gls{svara}. \Gls{chalan} can be considered as an abstraction of \gls{raga} motifs, and is a characterizing feature of \glspl{raga}. \acrshort{ragarecTDMS} utilizes \gls{chalan} aspect of melodies in \gls{iam} to perform \gls{raga} recognition.

In order to abstract the continuous melody representation and incorporate \gls{chalan} aspects, \acrshort{ragarecTDMS} uses a novel feature, the \acrfull{tdms}. \gls{tdms} captures tonal and temporal melodic aspects that are useful in characterizing and distinguishing \glspl{raga}. \Glspl{tdms} alleviate several of the shortcomings in the existing approaches (\secref{sec:sota_raga_recognition}) and improves the accuracy of \gls{raga} recognition by large margins. \Gls{tdms} is inspired by the concept of delay coordinates~\citep{takens1981detecting}. The main strengths of \gls{tdms} are:

\begin{itemize}
	\item It is a compact representation that describes both the tonal and the temporal characteristics of a melody
	\item It simultaneously captures the melodic characteristics at different time-scales, the overall usage of the pitch-classes in the entire recording, and the short-time temporal relation between individual pitches.
	\item It is robust to pitch octave errors.
	\item It does not require a \glspl{svara} level transcription of the melody nor its discrete representation.
	\item It has few parameters that are easy to tune (when set within reasonable bounds).
	\item It is easy to implement, fast to compute, and has a musically meaningful interpretation.
	\item As it will be shown, it obtains unprecedented accuracies in the \gls{raga} recognition task, outperforming the state of the art by a large margin, without the use of any elaborated classification schema.
\end{itemize}

We now proceed to describe \acrshort{ragarecTDMS}, which is based on the \gls{tdms} features. The computation of \gls{tdms} is described in the subsequent section (\secref{sec:tdms_feature_extraction}). The classification strategy and the distance measure used on top of the \gls{tdms} features for recognizing \glspl{raga} are described in~\secref{sec:tdms_classification_evaluation}. In~\secref{sec:tdms_evaluation}, we present the experimental setup used to evaluate this method, and finally, discuss the obtained results (\secref{sec:tdms_results_and_discussion}). 

\subsection{\titlecap{\glsentrylong{tdms}}}
\label{sec:tdms_feature_extraction}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeSeventy]{ch07_ragaRecognition/figures/tdms_computation.pdf}
	\end{center}
	\caption{Block diagram for \acrshort{tdms} computation.}
	\label{fig:bd_tdms_computation}
\end{figure}

The computation of a \gls{tdms} involves three steps as shown in~\figref{fig:bd_tdms_computation}: pre-processing, surface generation, and post-processing. In the pre-processing step, we obtain a low-level representation of the melody from an audio recording, which is then put to a meaningful tonal context by normalizing it with respect to the tonic pitch of the recording. In the surface generation step, we compute a two dimensional surface based on the concept of delay coordinates. Finally, in the post-processing step, we apply power compression and Gaussian smoothing to the computed surface. All these processing steps are described in the subsequent sections.

\subsubsection{Pre-processing} 
\label{sec:tdms_preprocessing}

\paragraph{Predominant Pitch Estimation:} In this step we process an audio recording to obtain a low-level representation of the melody, which is subsequently used in the computation of the \gls{tdms}. For this, we consider predominant pitch in the audio signal as the low-level melody representation. For predominant pitch estimation we follow the method described in~\secref{sec:data_preprocessing_predominant_melody_estimation}. In addition, we also post-process the estimated pitch to smoothen it and remove spurious pitch jumps (\secref{sec:data_preprocessing_pitch_postprocessing}).


\paragraph{Tonic Normalization:} The base frequency chosen in a performance of \gls{iam} is the tonic pitch of the lead artist, to which all other accompanying instruments are tuned~(\secref{sec:melody_in_iam}). 
Tonic pitch varies across artists and their recordings, and therefore, a meaningful feature for \gls{raga} recognition should be normalized with respect to the tonic pitch. To achieve this, we normalize the predominant pitch of every recording by considering its tonic pitch as the reference frequency during the Hertz-to-Cent-scale conversion~(\secref{sec:pre_processing_melody_representation}). The tonic pitch of the lead artist for every recording is automatically identified using \acrshort{tonicid_justin}, the method that outperformed all other existing methods for this task as shown in our comparative evaluation~(\secref{sec:pre_processing_tonic_identification_summary}). 

\subsubsection{Surface Generation}
\label{sec:tdms_surface_generation}

The next step is to construct a two-dimensional surface based on the concept of delay coordinates (also termed phase space embedding)~\citep{takens1981detecting, Kantz04BOOK}. In fact, such two-dimensional surface can be seen as a discretized histogram of the elements in a two-dimensional Poicar\'e map~\citep{Kantz04BOOK}. For a given recording, we generate a surface $\tdmsBase$ of size $\sizeTDMS\times\sizeTDMS$ by computing
\begin{equation}
\label{eq:surface_computation1}	
\tdmsElem_{ij} = \sum_{t=\timeDelay}^{N-1} \indFnc\left(\binOp\left(\pitchCents_t\right),i\right)~ \indFnc\left(\binOp\left(\pitchCents_{t-\timeDelay}\right),j\right) 
\end{equation}
for $0 \leq i,j < \sizeTDMS$, where $\tdmsElem_{ij}$ is the $(i,j)^\mathrm{th}$ element of the two-dimensional matrix $\tdmsBase$, $\pitchCents_t$ is the $t^\mathrm{th}$ sample (in Cent-scale) of the pitch sequence of length $N$, $\indFnc$ is an indicator function such that
\begin{equation}
\begin{aligned}
\indFnc(x,y)=
\begin{cases}
1, & \text{iff } x=y\\
0, & \text{otherwise}
\end{cases}
\end{aligned}
\end{equation} 
$\binOp$ is an octave-wrapping integer binning operator defined by 
\begin{equation}	
\label{eq:binning_function}	
\binOp(x) = \left\lfloor ~\left(\frac{\sizeTDMS x}{1200}\right) \bmod \sizeTDMS ~\right\rfloor,
\end{equation}
and $\timeDelay$ is a time delay index (in frames) that is left as a parameter. Note that, the frames where a predominant pitch could not be obtained (unvoiced segments) are excluded from any calculation. For the size of $\tdmsBase$ we use $\sizeTDMS = 120$. This value corresponds to 10 cents per bin, an optimal pitch resolution reported by~\cite{chordia2013joint}. In that study, the authors show that \gls{raga} recognition using \glspl{pcd} with a bin width of 10\,Cents outperforms the \glspl{pcd} with a bin width of 100\,Cents, and obtains a comparable results to \glspl{pcd} with a bin width of 5\,Cents.

An example of the generated surface $\tdmsBase$ for a music piece\footnote{\url{http://musicbrainz.org/recording/e59642ca-72bc-466b-bf4b-d82bfbc7b4af}} in \gls{raga} Yaman is shown in~\figref{fig:phase_space_surface}\,(a). We see that the prominent peaks in the surface correspond to the \glspl{svara} of \gls{raga} Yaman. We notice that these peaks are steep and the dynamic range of the surface is high. This can be attributed to the nature of the melodies in these music traditions, particularly in Hindustani music, where the melodies often contain long held \glspl{svara}. In addition, the dynamic range is high also because the pitches in the stable \gls{svara} regions lie within a small frequency range around the mean \gls{svara} frequency compared to the pitches in the transitory melodic regions. Because of this, most of the pitch values are mapped to a small number of bins, making the prominent peaks more steep.

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch07_ragaRecognition/figures/PSfeature_e59642ca-72bc-466b-bf4b-d82bfbc7b4af.pdf}
	\end{center}
	\caption[\Acrfull{tdms} before and after post-processing]{Generated surface for a music piece before (a) and after (b) applying post-processing ($\tdmsBase$ and $\tdmsSmooth$, respectively). For ease of visualization, both matrices are normalized here between 0 and 1.}
	\label{fig:phase_space_surface}
\end{figure}

\subsubsection{Post-processing}
\label{sec:tdms_post_processing}

\paragraph{Power Compression:} In order to accentuate the values corresponding to the transitory regions in the melody and reduce the dynamic range of the surface, we apply an element-wise power compression
\begin{equation}
\label{eq:tdms_power_compression}	
\tdmsPower= \tdmsBase^\tdmsPowFac ,
\end{equation}
where $\tdmsPowFac$ is an exponent that is left as a parameter. Once a more compact (in terms of the dynamic range) surface is obtained, we apply Gaussian smoothing. With that, we attempt to attenuate the subtle differences in $\tdmsPower$ corresponding to the different melodies within the same \gls{raga}, while retaining the attributes that characterize that \gls{raga}. 

\paragraph{Gaussian Smoothening:} We perform Gaussian smoothing by circularly convolving $\tdmsPower$ with a two-dimensional Gaussian kernel. We choose a circular convolution because of the cyclic (or octave-folded) nature of the \gls{tdms}~(\eqnref{eq:binning_function}), which mimics the cyclic nature of pitch-classes. The standard deviation of this kernel is $\sigmaTDMS$ bins (samples). The length of the kernel is truncated to $8\sigmaTDMS + 1$ bins in each dimension, after which the values are negligible (below $0.01\%$ of the kernel's maximum amplitude). We experiment with different values of $\sigmaTDMS$, and also with a method variant excluding the Gaussian smoothing (loosely denoted by $\sigmaTDMS = -1$), so that we can quantify its influence on the accuracy of the system. 
%
%After convolution we retain the same dimensionality of the surface as before by discarding $4\sigmaTDMS$ number of samples from the start and the end of each dimension (i.e.~the border). 

Once we have the smoothed surface $\tdmsSmooth$, there is only one step remaining to obtain the final \gls{tdms}. Since the overall duration of the recordings and of the voiced regions within them is different, the computed surface $\tdmsSmooth$ needs to be normalized. To do so, we divide $\tdmsSmooth$ by its $\mathrm{L}1$-norm:
\begin{equation}
\label{eq:tdms_normalization}
\tdmsNorm = \tdmsSmooth/\norm{\tdmsSmooth}_{1} .
\end{equation}
This also yields values of $\tdmsNorm$, the final \gls{tdms}, that are interpretable in terms of discrete probabilities.

The result after post-processing the surface in~\figref{fig:phase_space_surface}\,(a) with power compression and Gaussian smoothing is shown in~\figref{fig:phase_space_surface}\,(b). We see that the values corresponding to the non-diagonal elements are accentuated. A visual inspection of~\figref{fig:phase_space_surface}\,(b) provides several musical insights to the melodic aspects of the recording. For instance, the high salience indices along the diagonal, $(0,0)$, $(20,20)$, $(40,40)$, $(60,60)$, $(70,70)$, $(90,90)$, and $(110,110)$, correspond to the 7~\glspl{svara} used in \gls{raga} Yaman. Within which, the highest salience at indices (110,110) correspond to the \gls{ni} \gls{svara}, which is the \gls{vadi} \gls{svara}, that is, musically the most salient \gls{svara} of the \gls{raga}, in this case \gls{raga} Yaman~\citep{rao1999raga}. The asymmetry in the matrix with respect to the diagonal indicates the asymmetric nature of the ascending and descending \gls{svara} progression, \gls{arohana}-\gls{avrohana}, of the \gls{raga} (compare, for example, the salience at indices $(70, 90)$ to indices $(90, 70)$, with the former being more salient than the latter). The similarity of the matrix between indices $(20,20)$ and $(70,70)$ with respect to the matrix between indices $(70,70)$ and $(120,120)$ delineates the tetra-chord structure of the \gls{raga}. Thus, we see that the \gls{tdms} captures several features related with the tonal and the temporal aspects of melody at different time-scales. Finally, it should be noted that an interesting property of the \gls{tdms} feature is that the mean of the sum across its rows and columns yields a \gls{pcd} representation, widely used in \gls{raga} recognition~(\secref{sec:sota_raga_recognition}).

%\COMMENT{If time permits put the points above in bullets. That would help to clearly understand the points.Also add a point about the vertical + horizontal blurring/smoothened nature of the surface around Pa svar. This indicate the slow gliding movement. Similarly for Ga (40,40) it can be said that its hit bang on without any glide.}


\subsection{Evaluation}
\label{sec:tdms_evaluation}

\subsubsection{Music Collection}
\label{sec:tdms_music_collection}

To evaluate \acrshort{ragarecTDMS} method we use the same music collection as used in the evaluation of \acrshort{ragarecVSM}~(\secref{sec:raga_rec_pattern_music_collection}). It comprises two datasets,\acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}, one for each music tradition, Carnatic and Hindustani music, respectively. As mentioned before, a separate evaluation on both the music traditions allows a better analysis and interpretation of the results. 

To reiterate, these are the largest datasets ever used for studying the task of automatic \gls{raga} recognition. To facilitate reproducible research and comparative studies we also make these datasets publicly available online (\appref{app:resources}).

%\acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} comprise 124 and 130~hours of commercially available audio recordings, respectively. \acrshort{rrds_cmd_big} contains full-length recordings of 480~performances belonging to 40~\glspl{raga} with 12~music pieces per \gls{raga}. \acrshort{rrds_hmd_big} contains full-length recordings of 300~performances belonging to 30~\glspl{raga} with 10~music pieces per \gls{raga}. The selected music material is diverse and is representative of these music traditions. A detailed description of these datasets is provided in~\secref{sec:corpus_raga_recognition_datasets}.




\subsubsection{Classification and Distance Measures}
\label{sec:tdms_classification_evaluation}

In order to demonstrate the ability of the \glspl{tdms} in capturing \gls{raga} characteristics, we consider the task of classifying audio recordings according to their \gls{raga} label. To perform classification, we choose a \acrfull{knn} classifier~\citep{Mitchell97BOOK}. The reasons for our choice are manifold. Firstly, the \gls{knn} classifier is well understood, with well studied relations to other classifiers in terms of both performance and architecture. Secondly, it is fast, with practically no training and with known techniques to speed up testing or retrieval. Thirdly, it has only one parameter, $k$, which we can just blindly set to a relatively small value or can easily optimize in the training phase. Finally, it is a classifier that is simple to implement and whose results are both interpretable and easily reproducible. 

The performance of a \gls{knn} classifier highly depends on the distance measure used to retrieve the $k$ neighbors. We consider three different measures to compute the distance between two recordings $n$ and $m$ with \gls{tdms} features $\tdmsNorm^{(n)}$ and $\tdmsNorm^{(m)}$, respectively. We first consider the Frobenius norm of the difference between $\tdmsNorm^{(n)}$ and $\tdmsNorm^{(m)}$,
\begin{equation}
\distTDMS_{\mathrm{F}}^{(n,m)} = \norm{\tdmsNorm^{(n)} - \tdmsNorm^{(m)}}_2 .
\end{equation}
Next, we consider the symmetric Kullback-Leibler divergence
\begin{equation}
\distTDMS_{\mathrm{KL}}^{(n,m)} = D_{\mathrm{KL}}\left(\tdmsNorm^{(n)},\tdmsNorm^{(m)}\right) + D_{\mathrm{KL}}\left(\tdmsNorm^{(m)},\tdmsNorm^{(n)}\right) ,
\end{equation}
with
\begin{equation}
D_{\mathrm{KL}}\left(\mathbf{X},\mathbf{Y}\right) = \sum{\mathbf{X} \log\left(\frac{\mathbf{X}}{\mathbf{Y}} \right)} ,
\end{equation}
where we perform element-wise operations and sum over all the elements of the resultant matrix. Finally, we consider the Bhattacharyya distance, which is reported to outperform other distance measures with a \gls{pcd}-based feature for the same task in~\cite{chordia2013joint},
\begin{equation}
\distTDMS_{\mathrm{B}}^{(n,m)} = -\log\left( \sum{ \sqrt{ \tdmsNorm^{(n)} \cdot \tdmsNorm^{(m)} } } \right) .
\end{equation}
We again perform element-wise operations and sum over all the elements of the resultant matrix. Variants of our proposed method that use $\distTDMS_{\mathrm{F}}$, $\distTDMS_{\mathrm{KL}}$ and $\distTDMS_{\mathrm{B}}$ are denoted by \acrshort{ragarecTDMS_F}, \acrshort{ragarecTDMS_KL}, and \acrshort{ragarecTDMS_B}, respectively.


\subsubsection{Comparison with Other Methods}
\label{sec:tdms_comparison_other}

In addition to \acrshort{ragarecTDMS}, we also evaluate and compare the method proposed by~\cite{chordia2013joint} (denoted by \acrshort{sotaChordia}) in the same way as explained in~\secref{sec:raga_rec_pattern_comparison_sota}. \acrshort{sotaChordia} is regarded as the state of the art in \gls{raga} recognition, which uses smoothened \acrfull{pcd} as the tonal feature and employs \acrfull{1nn} using Bhattacharyya distance for predicting \gls{raga} labels. We use the original implementation of these method obtained from the respective author. We also compare the performance of \acrshort{ragarecTDMS} with our melodic pattern-based method, \acrshort{ragarecVSM}~(\secref{sec:pattern_based_raga_recognition}).


\subsubsection{Experimental Setup}
\label{sec:tdms_experimental_setup}

To evaluate \acrshort{ragarecTDMS} we use the same experimental setup as used in the evaluation of \acrshort{ragarecVSM}~(\secref{sec:raga_rec_pattern_classification_evaluation}). We perform a leave-one-out cross validation~\citep{Mitchell97BOOK}, in which one recording from the evaluation data set forms the testing set and the remaining ones become the training set. To quantify the performance of the considered methods we use the raw overall accuracy~\citep{Mitchell97BOOK}. Since both \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} are balanced datasets in the number of instances per class, we do not need to correct such raw accuracies to counteract for possible biases towards the majority class. To assess if the difference in the performance between any two methods is statistically significant, we use McNemar's test~\citep{mcnemar1947note} with $\pVal < 0.01$. To compensate for multiple comparisons, we apply the Holm-Bonferroni method~\citep{holm1979simple}. Besides accuracy, and for a more detailed error analysis, we also compute the confusion matrix over the predicted classes.

In the case of \acrshort{ragarecTDMS}, a test recording is assigned the majority class of its $k$-nearest neighbors obtained from the training set and, in case of a tie, one of the majority classes is selected randomly. Because we conjecture that none of the parameters we consider is critical to obtain a good performance, we initially make an educated guess and intuitively set our parameters to a specific combination. We later study the influence of every parameter starting from that combination. We initially use $\timeDelay=0.3$\,s, $\tdmsPowFac=0.75$, $\sigmaTDMS=2$, and $k=1$, and later consider $\timeDelay\in\lbrace 0.2, 0.3, 0.5, 1, 1.5\rbrace$\,s, $\tdmsPowFac\in\lbrace 0.1, 0.25, 0.5, 0.75, 1 \rbrace$, $\sigmaTDMS\in\lbrace -1, 1, 2, 3\rbrace$, and $k\in\lbrace 1,3,5\rbrace$ (recall that $\sigmaTDMS=-1$ corresponds to no smoothing~\secref{sec:tdms_post_processing}).



\subsection{Results and Discussion}
\label{sec:tdms_results_and_discussion}

In Table~\ref{tab:main_results_tdms}, we show the results for all the variants of the proposed method \acrshort{ragarecTDMS_F}, \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B}, and for \acrshort{sotaChordia} and \acrshort{ragarecVSM}, using \acrshort{rrds_hmd_big} and \acrshort{rrds_cmd_big} datasets. We see that the highest accuracy obtained on \acrshort{rrds_hmd_big} is 97.7\% by \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B}. This accuracy is considerably higher than the 91.7\% obtained by \acrshort{sotaChordia}, and the difference is found to be statistically significant. We also see that \acrshort{sotaChordia} performs significantly better than \acrshort{ragarecVSM}. Regarding the proposed variants, we see that, in \acrshort{rrds_hmd_big}, \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B} perform better than \acrshort{ragarecTDMS_F}, with a statistically significant difference. 

\begin{table}
	\ra{1.3}
	\centering
	{
		\begin{tabular}{ c | c c c | c c c}
\tabletop
			Dataset   	& 	\acrshort{ragarecTDMS_F} 	&	\acrshort{ragarecTDMS_KL}		&	\acrshort{ragarecTDMS_B}	&	\acrshort{ragarecVSM}		& \acrshort{sotaChordia}& \acrshort{sotaKoduri}	\\	
\tablemid
			\acrshort{rrds_hmd_big}   	& 	91.3 	&	{\bf 97.7}		&	{\bf 97.7} 	&	83.0		&	91.7	& NA\\	
			
			\acrshort{rrds_cmd_big}   	& 	81.5	&	{\bf 86.7}		&	{\bf 86.7}	&	67.3		&	73.1	& 54.8\\	
\tablebot
		\end{tabular}
	}
	\caption[\Gls{raga} recognition accuracy of the \acrshort{tdms}-based method variants
	]{Accuracy (\%) of the three proposed variants, \acrshort{ragarecTDMS_F}, \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B}, and other methods \acrshort{ragarecVSM}, \acrshort{sotaChordia} and \acrshort{sotaKoduri}. The random baseline ($\mathfrak{B}_\mathrm{r}$) for this task is 3.3\% for \acrshort{rrds_hmd_big} and 2.5\% for \acrshort{rrds_cmd_big}. NA stands for not applicable.}
	\label{tab:main_results_tdms}
\end{table}

In Table~\ref{tab:main_results_tdms}, we also see that the trend in the performance for \acrshort{rrds_cmd_big} across different methods is similar to that for \acrshort{rrds_hmd_big}. The variants \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B} achieve the highest accuracy of 86.7\%, followed by \acrshort{sotaChordia} with 73.1\%. The difference in performance between both the methods, \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B}, and \acrshort{sotaChordia} is found to be statistically significant. For \acrshort{rrds_cmd_big}, also \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B} perform better than \acrshort{ragarecTDMS_F}, with a statistically significant difference. 

In general, we notice that, for every method, the accuracy is higher on \acrshort{rrds_hmd_big} compared to \acrshort{rrds_cmd_big}. This, as expected, can be largely attributed to the difference in the number of classes in \acrshort{rrds_hmd_big} (30 \glspl{raga}) and \acrshort{rrds_cmd_big} (40 \glspl{raga}). A higher number of classes makes the task of \gls{raga} recognition more challenging for \acrshort{rrds_cmd_big}, compared to \acrshort{rrds_hmd_big}. In addition to that, another factor that can cause this difference could be the length of the audio recordings, which for \acrshort{rrds_hmd_big} are significantly longer than the ones in \acrshort{rrds_cmd_big}~(\secref{sec:corpus_raga_recognition_datasets}).

As mentioned earlier, the system parameters corresponding to the results in Table~\ref{tab:main_results_tdms} were set intuitively, without any parameter tuning. Since \glspl{tdms} are novel features that are being used for the first time, we want to carefully analyze the influence that each of the parameters has on the final \gls{raga} recognition accuracy, and ultimately perform a quantitative assessment of their importance. In~\figref{fig:accuracy_vs_parameter_values_tdms}, we show the accuracy of \acrshort{ragarecTDMS_KL} for different values of these parameters. In each case, only one parameter is varied and the rest are set to the initial values mentioned above. 

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch07_ragaRecognition/figures/AccuracyVsParameters_tdms.pdf}
	\end{center}
	\caption[Accuracy of \acrshort{ragarecTDMS_KL} as a function of different parameter values]{Accuracy of \acrshort{ragarecTDMS_KL} as a function of parameter values. Other methods \acrshort{sotaChordia} and \acrshort{ragarecVSM}, and random baseline $\mathfrak{B}_\mathrm{r}$ are also reported for comparison.} 
	\label{fig:accuracy_vs_parameter_values_tdms}
\end{figure}

In~\figref{fig:accuracy_vs_parameter_values_tdms}\,(a), we observe that the performance of the method is quite invariant to the choice of $\timeDelay$, except for the extreme delay values of 1 and 1.5\,s for \acrshort{rrds_cmd_big}. This can be attributed to the melodic characteristics of Carnatic music, which presents a higher degree of oscillatory melody movements and shorter stationary \gls{svara} regions, as compared to Hindustani music. In~\figref{fig:accuracy_vs_parameter_values_tdms}\,(b), we see that compression with $\tdmsPowFac < 1$ slightly improves the performance of the method for both datasets. However, the performance degrades for $\tdmsPowFac < 0.75$ for \acrshort{rrds_cmd_big} and $\tdmsPowFac < 0.25$ for \acrshort{rrds_hmd_big}. This again appears to be correlated with the long steady nature of the \glspl{svara} in Hindustani music melodies. Because the dynamic range of $\tdmsBase$ is high, \gls{tdms} features require a lower value for the compression factor $\tdmsPowFac$ to accentuate the surface values corresponding to the transitory regions in the melodies of Hindustani music. In~\figref{fig:accuracy_vs_parameter_values_tdms}\,(c), we observe that Gaussian smoothing significantly improves the performance of the method, and that such performance is invariant across the chosen values of $\sigmaTDMS$. Finally, in Figure~\ref{fig:accuracy_vs_parameter_values_tdms}\,(d), we notice that the accuracy decreases with increasing $k$. This is also expected due to the relatively small number of samples per class in our datasets~\citep{Mitchell97BOOK}. The best accuracy obtained is for $k=1$. A similar observation is also reported in a previous study that uses \gls{pcd}-based feature for the same task~\citep{chordia2013joint}. 

Overall, the method appears to be invariant to different parameter values to a large extent, which implies that it is easy to extend and tune it to other datasets. It is important to note that, no matter what parameter configuration we use, the accuracy of \acrshort{ragarecTDMS_KL} never goes below the baselines' accuracies (see~\figref{fig:accuracy_vs_parameter_values_tdms}).

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\figSizeNinety]{ch07_ragaRecognition/figures/CM_tdms_cmd_var1.pdf}
	\end{center}
	\caption[Confusion matrix of classification results by \acrshort{ragarecTDMS_KL} on \acrshort{rrds_cmd_big}]{Confusion matrix of the predicted \gls{raga} labels obtained by \acrshort{ragarecTDMS_KL} on \acrshort{rrds_cmd_big}. Shades of grey are mapped to the number of audio recordings.} 
	\label{confusion_mtx_carnatic_tdms}
\end{figure}

From the results reported in~\figref{fig:accuracy_vs_parameter_values_tdms}, we see that there exist a number of parameter combinations that could potentially yield a better accuracy than the one reported in~\tabref{tab:main_results_tdms}. For instance, using $\timeDelay=0.3$\,s, $\tdmsPowFac=0.5$, $\sigmaTDMS=2$, and $k=1$, we are able to reach 97.0\% for \acrshort{ragarecTDMS_F} and 98.0\% for both \acrshort{ragarecTDMS_KL} and \acrshort{ragarecTDMS_B} on \acrshort{rrds_hmd_big}. These accuracies are ad-hoc, optimizing the parameters on the testing set. However, and doing things more properly, we could learn the optimal parameters in training, through a standard grid search, cross-validated procedure over the training set~\citep{Mitchell97BOOK,hastie2009unsupervised}. As our primary goal here is not to obtain the best possible results, but to show the usefulness and superiority of \glspl{tdms}, we do not perform such an exhaustive parameter tuning and leave it for future research.

We now proceed to analyze the errors made by the best performing variant \acrshort{ragarecTDMS_KL}. For \acrshort{rrds_cmd_big}, we show the confusion matrix of the predicted \gls{raga} labels in Figure~\ref{confusion_mtx_carnatic_tdms}. In general, we see that the confusions have a musical explanation. The majority of them are between the \glspl{raga} in the sets $\lbrace$\gls{bhairavi}, \gls{mukhari}$\rbrace$, $\lbrace$\gls{harikambhoji}, \gls{kambhoji}$\rbrace$, $\lbrace$\gls{madhyamavati}, \gls{atana}, \gls{sri}$\rbrace$, and $\lbrace$\gls{kapi}, \gls{anandabhairavi}$\rbrace$. \Glspl{raga} within each of these sets are allied \glspl{raga}~\citep{Viswanathan2004}~(\secref{sec:melody_in_iam}), i.e., they share a common set of \glspl{svara} and similar phrases. Due to these characteristics, distinguishing between allied \glspl{raga} is a challenging task, which is often based on subtle melodic nuances.

For \acrshort{rrds_hmd_big}, there are only seven incorrectly classified recordings~(\figref{fig:confusion_matrix_hmd_tdms}). Confusions between \Gls{raga} \gls{alahaiya_bilaval}, \gls{des} and \gls{khamaj} is explicable as these \glspl{raga} share exactly the same set of \glspl{svara}. Similar is the case between \gls{raga} \gls{des} and \gls{gaud_malhar}, wherein both the \glspl{raga} belong to the malh\={a}r group of \glspl{raga} and have similar melodic phrases. For two specific cases of confusions, that of \gls{raga} \gls{khamaj} with \gls{bagesri}, and \gls{raga} \gls{darbari} with \gls{bhup}, we find that the error lies in the estimation of the tonic pitch.

\section{Effect of Dataset on Accuracy}
\label{sec:ragarec_dataset_effect}

From our review of the current approaches for \gls{raga} recognition in~\secref{sec:sota_raga_recognition}, we see that their direct comparison in terms of the performance is not meaningful. This is mainly due to the diversity in the datasets using which they are evaluated~(\tabref{tab:raga_recognition_methods_details}). These datasets differ in terms of the number of \glspl{raga} , specific set of chosen \glspl{raga}, the number of recordings per \gls{raga} and the type of audio (monophonic or polyphonic). A comprehensive comparison of the performance of these approaches requires a common evaluation on the same dataset and experimental setup, which is an arduous task. However, to begin with, even a systematic assessment of the influence of a dataset on \gls{raga} recognition accuracy for a particular method can provide us several useful insights. Certainly, the trend in the accuracy across different dataset variants will be dependent on the chosen method. However, we hypothesize that using one of the most robust and the best performing methods will minimize this influence. In addition, we believe that it will act as a lower bound on the variations in the accuracies across the dataset variants. 

For this experiment we consider \acrshort{ragarecTDMS_KL} method, which as shown above achieves the highest accuracy on both the datasets, \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}. We want to assess the influence of the number of \glspl{raga} in the dataset and the specific combination of \glspl{raga} in a set. To achieve this, we perform evaluations on randomly sampled subsets of both the datasets. We randomly select sets of 5, 10, 20, and 30\,\glspl{raga} from \acrshort{rrds_cmd_big} dataset that comprise 40 \glspl{raga} with 12\,recordings per raga. We maintain the same number of recordings (12) per \gls{raga} in all the sets. For \acrshort{rrds_hmd_big} dataset, the sizes of the subsets are 5, 10, 15, 20 and 25. We repeat this entire process of random selection 50 times and ensure that at each iteration the new subsets are maximally different from the previous ones in terms of the choice of \gls{raga}. We now proceed to present the results of our evaluations. 


\begin{figure}
	\begin{subfigure}{\textwidth}
		\centering
		\ifdefined\PRINTVER
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/Accuracy_Vs_Size_n_raagas_cmd_BW.pdf}
		\else
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/Accuracy_Vs_Size_n_raagas_cmd.pdf}
		\fi
		\caption{For subsets of \acrshort{rrds_cmd_big} dataset}
		\label{fig:performance_vs_n_raagas_cmd}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\ifdefined\PRINTVER
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/Accuracy_Vs_Size_n_raagas_hmd_BW.pdf}
		\else
			\includegraphics[width=\figSizeEightyFive]{ch07_ragaRecognition/figures/Accuracy_Vs_Size_n_raagas_hmd.pdf}
		\fi
		\caption{For subsets of \acrshort{rrds_hmd_big} dataset}
		\label{fig:performance_vs_n_raagas_hmd}
	\end{subfigure}
	\caption[Accuracy of \acrshort{ragarecVSM} as a function number of \glspl{raga} in a subset of \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}]{Accuracy of \acrshort{ragarecVSM} as a function number of \glspl{raga} in a subset of \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big}, respectively. For every case 50 randomly sampled selections of \glspl{raga} are done from the original datasets.}
	\label{fig:performance_vs_n_raagas}
\end{figure}

In~\figref{fig:performance_vs_n_raagas} we show a boxplot of the \gls{raga} recognition accuracies obtained for different subsets of the datasets over multiple iterations of the experiment. In general we observe that the accuracy decreases as the number of \glspl{raga} in the dataset increase. This is explicable as the task becomes more and more challenging. Notably, the drop in the performance is more significant in the case of the subsets comprising Carnatic music~(\figref{fig:performance_vs_n_raagas}\,(a)) as compared to Hindustani music. In addition, the median accuracies on these subsets is lower. Both these factors indicate that the task of \gls{raga} recognition is more challenging in the case of audio recordings in Carnatic music as compared to Hindustani music, and that more number of \glspl{raga} makes it even more difficult for the former. 

Another interesting aspect is to look at is the performance across multiple iterations, i.e.~across different sets of a fix number of \glspl{raga}. We see that the accuracy varies by around 10\,\% for the case of Carnatic music dataset. This means that the accuracy of a method for the same number of \glspl{raga} and the same number of recordings per \gls{raga} can vary by 10\% just because of a different selection of \glspl{raga}. It strongly implies that that the complexity of the task of \gls{raga} recognition depends on the \glspl{raga} being differentiated. Thus, these factors should be taken into account while interpreting the results of a method in \gls{raga} recognition task. Note that for Hindustani music such variation is not as significant. 

Overall, we see that the accuracy of \gls{raga} recognition is sensitive to a dataset, both in terms of the number of \glspl{raga} and the specific set of chosen \glspl{raga}. We notice that the sensitivity is more for Carnatic music dataset as compared to Hindustani music dataset. An important takeaway of this experiment is the degree to which the accuracy varies across the different dataset variants. This will help us to better interpret and compare the results of the existing studies on \gls{raga} recognition. 


\section{Summary and Conclusions}
\label{sec:conclusions_raga_recognition}

In this chapter, we presented two computational approaches for automatically recognizing \glspl{raga} in audio collections of \gls{iam}, which address a number of shortcomings in the existing approaches identified in our literature review. They utilize both the tonal and the temporal characteristics of melodies for \gls{raga} recognition, and require only an estimate of the predominant pitch and the tonic of the performance from audio recordings.

We first described our pattern-based method for \gls{raga} recognition (\acrshort{ragarecVSM}), which uses melodic patterns discovered in an unsupervised manner by our approach presented in the previous chapter. In order to group the patterns that represent the same melodic phrase we clustered them using a non-overlapping community detection method applied on a network built using the discovered patterns. In this process we also removed connections between the melodically dissimilar patterns by applying a similarity threshold, which we estimated by exploiting the topological properties of the network. We subsequently employed a vector space model to represent audio recordings in terms of these melodic patterns. The \gls{tfidf}-based features thus obtained are then used to train a classifier to predict \gls{raga} labels. For evaluating and comparing our method and the state of the art methods we used the sizable Carnatic and Hindustani music collection we compiled and curated for this task. To the best of our knowledge, these are the largest datasets ever used for evaluating \gls{raga} recognition methods. We experimented with a number of classification algorithms and found that the multinomial naive Bayes classifier outperforms the rest. We showed that our pattern-based \gls{raga} recognition approach that utilize vector space modeling concepts is a successful strategy, yielding comparable accuracies to the state of the art. We see that \acrshort{ragarecVSM} achieves an accuracy of 67.29\% and 82.66\% for \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} datasets, respectively. The accuracy obtained by the state of the art method is 73.12\% and 91.6\% for \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} datasets, respectively. 

An analysis of the classification errors revealed that the majority of the confusions occurred between the allied \glspl{raga}, which are differentiated by experts based on subtle melodic nuances. In addition, we found that the errors made by the pattern-based and the \gls{pcd}-based methods are complementary, and thus, they can be combined to improve the accuracy in \gls{raga} recognition. Being a phrase-based approach, there are several advantages of \acrshort{ragarecVSM}: the features used by this method for classification have musically meaningful interpretations, the feature weight assigned by the classifier can indicate the relative importance of different melodic patterns in distinguishing between the \glspl{raga}, and finally, since this approach does not require a pitch distribution, it can be used in a real-time \gls{raga} recognition setup. To the best of our knowledge, no other method to date has employed a fully automated methodology for discovering and selecting musically relevant melodic patterns for \gls{raga} recognition on this scale. The evaluation of \acrshort{ragarecVSM} can also be regarded as an indirect quantitative evaluation of our pattern discovery approach, which based on our results, suggest that the discovered melodic patterns are musically relevant.

We subsequently described our second method (\acrshort{ragarecTDMS}) that uses a novel feature, the \acrshort{tdms}, which captures both the tonal and the short-time temporal aspects of a melody relevant in \gls{raga} characterization. This feature is derived directly from the tonic-normalized predominant pitch in the audio. A visual inspection of the \acrshort{tdms} revealed interesting musical insights. We demonstrated the capabilities of \glspl{tdms} in capturing \gls{raga} characteristics by classifying audio recordings according to their \glspl{raga} labels. For evaluating this method we use the same music collections as mentioned above. We showed that using a \gls{knn} classifier, the proposed feature outperformed the state of the art methods in \gls{raga} recognition with unprecedented accuracies. We see that \acrshort{ragarecTDMS} achieves an accuracy of 86.7\% and 97.7\% for \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} datasets, respectively. The accuracy obtained by the state of the art method is 73.1\% and 91.7\% for \acrshort{rrds_cmd_big} and \acrshort{rrds_hmd_big} datasets, respectively. \acrshort{ragarecTDMS} has several advantages: it is robust to pitch octave errors, it does not require a transcribed melody representation, it involves only a few parameters, it is easy to implement and fast to compute. We also studied the influence of different parameters on the accuracy obtained by \glspl{tdms}, and found that it is largely invariant to different parameter values. This indicates that it is easy to extend and tune this method to other datasets. An analysis of the classification errors revealed that the confusions occur between musically similar \glspl{raga} that share a common set of \glspl{svara} and have similar melodic phrases. 

We also analyzed the influence of a dataset in terms of the number and selection of \glspl{raga} on the complexity of the \gls{raga} recognition task. For this analysis we selected the best performing method (\acrshort{ragarecTDMS}). Our results indicated that this task is more challenging for audio recordings in Carnatic music as compared to Hindustani music. We showed that an increase in the number of \glspl{raga} in the dataset reduces the prediction accuracy of the method more severely in the case of Carnatic music than for Hindustani music. We also showed that for the same number of \glspl{raga} in a dataset, the selection of \glspl{raga} influences considerably the complexity of this task. We saw that the accuracy for the Carnatic music dataset can vary up to 10\% across different selections of \glspl{raga} in the dataset. The insights obtained in this analysis will help to better interpret the performance of the existing methods, which are typically evaluated on different datasets. In the future, we would like to investigate the minimum duration of the audio recording required to reliably recognize \glspl{raga}. Another promising direction for the future work is to explore methodologies that can combine \gls{pcd}-based, pattern-based, and \gls{tdms}-based methods to improve \gls{raga} recognition.

