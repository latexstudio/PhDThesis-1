%!TEX root = ../thesis_a4.tex

\chapter{\titlecap{Automatic \glsentrytext{raga} Recognition}}


%0) What is automatic raga identification
%1) Motivation and relevance of the problem
%2) References to thousands of papers that cater to this task
%3) What lacks in them
%4) How do we fill in the missing gaps
%5) What we present in this chapter
%6) What are the papers which this chapter is based on


\section{Introduction}

In this chapter we address the task of automatic \gls{raga} recognition, which is one of the most studied topics within \gls{mir} of Indian art music. We begin by highlighting the shortcomings of the existing \gls{raga} recognition approaches and subsequently present two different methods to address these limitations. We evaluate the proposed methods on representative and sizable collection of Hindustani and Carnatic music, both of which are made publicly available. In addition to evaluating our proposed methods we also compare their performance with the state of the art approaches by evaluating them under the same experimental conditions for the same data set.  

\TODO{Do we need to write what is automatic raga recognition?}

As mentioned before \gls{raga} is a core musical concept used in the composition, performance, organization, and pedagogy of Indian art music (IAM). Numerous compositions in Indian folk and film music are also based on \glspl{raga}~\citep{ganti2013bollywood}. Despite its significance in IAM, there exists a large volume of audio content whose \gls{raga} is incorrectly labeled or, simply, unlabeled. This is partially because the vast majority of the tools and technologies that interact with the recordings' metadata fall short of fulfilling the specific needs of the Indian music tradition~\citep{XavierSerra2011}. A computational approach to automatic \gls{raga} recognition can enable \gls{raga}-based music retrieval from large audio collections, semantically-meaningful music discovery, musicologically-informed navigation, as well as several applications around music pedagogy. 

\Gls{raga} recognition as mentioned earlier is widely studied topics within \gls{mir} of IAM. As a consequence, there exist a considerable amount of approaches utilizing different characteristic aspects of \glspl{raga}. A summary of these approaches is already provided in Section XXX. A large number of approaches for \gls{raga} recognition use features derived from the pitch or pitch-class distribution as can be seen in Table~XXXX. Using these features they capture the overall usage of the tonal material in an audio recording. In general, PCD-based approaches are robust to pitch octave errors, which is one of the most frequent errors in the estimation of predominant melody from polyphonic music signals. Currently, the PCD-based approach represents the state-of-the-art in \gls{raga} recognition~\citep{chordia2013joint}. Along with the merits there are several limitations of the these approaches. One of the major shortcomings of the PCD-based approaches is that they completely disregard the temporal aspects of the melody, which are essential to \gls{raga} characterization~\cite{rao1999raga}. Temporal aspects are even more relevant in distinguishing phrase-based \glspl{raga}~\cite{krishna2012carnatic}, as their aesthetics and identity is largely defined by the usage of meandering melodic movements, called gamakas. 

Several approaches address this shortcoming in the PCD-based approaches by modeling the temporal aspects of a melody in a variety of ways~\cite{kumar2014identifying, shetty2009raga, rajkumar2011identification}. Such approaches typically use melodic progression templates~\cite{shetty2009raga}, n-gram distributions~\cite{kumar2014identifying}, or hidden Markov models~\cite{rajkumar2011identification} to capture the sequential information in the melody. With that, they primarily utilize the \={a}r\={o}hana-avr\={o}hana pattern of a \gls{raga}. In addition, most of them either transcribe the predominant melody in terms of a discrete svara sequence, or use only a single symbol/state per svara. Thus, they discard the characteristic melodic transitions between svaras, which are a representative and distinguishing aspect of a \gls{raga}~\cite{rao1999raga}. Furthermore, they often rely on an accurate transcription of the melody, which is still a challenging and an ill-defined task given the nature of IAM~\cite{rao2012culture, Suvarnalata2014}. 

As mentioned before melodic patterns are one of the most prominent cues for \gls{raga} identification, used both by a performer and a listener~\citep{rao1999raga}. Despite the importance there do not exist many approaches that exploit these patterns for automatic \gls{raga} recognition. This can be attributed to the challenges involved in the extraction of characteristic melodic phrases in audio music collections. To the best of our knowledge there exist only two approaches so far that use melodic patterns for this task~\cite{shrey_ISMIR_2015, sridhar2009raga}, out of which only the former considers a continuous melodic contour for melodic phrases. Their aim is to create dictionaries of characteristic melodic phrases and to utilize them in the recognition phase. \TODO{What are the critical remarks for these two approaches?}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch07_ragaRecognition/figures/bd_overall_phasebased_raga_recognition.pdf}
	\end{center}
	\caption{Overall block diagram for automatic \gls{raga} recognition.}
	\label{fig:bd_raga_recognition}
\end{figure}



In this chapter we describe two novel methods $\mathcal{M}_\mathrm{\acrshort{vsm}}$~\TODO{Citations for both} and $\mathcal{M}_\mathrm{\acrshort{tdms}}$ that overcome a number of shortcomings in the existing approaches described above and improve the accuracy of \gls{raga} recognition. A general block diagram for both the methods is shown in Figure~\ref{fig:bd_raga_recognition}. There are two main processing blocks; feature extraction and classification. Given sets of audio recordings for training ($\mathrm{R}_\mathrm{tr}$) and testing ($\mathrm{R}_\mathrm{ts}$) we first extract features ($\mathrm{F}_\mathrm{tr}$ and $\mathrm{F}_\mathrm{ts}$) for every audio recording in both the sets. Subsequently features of the training set $\mathrm{F}_\mathrm{tr}$ along with the \gls{raga} labels $\mathrm{L}_\mathrm{tr}$ are used to train a classifier to build a classification model. Using this model we then predict the \gls{raga} labels for the testing set $\mathrm{L}_\mathrm{ts}$. $\mathcal{M}_\mathrm{\acrshort{vsm}}$ and $\mathcal{M}_\mathrm{\acrshort{tdms}}$ differ both in terms of the extracted features and the classification strategy. Both these methods are described at length in subsequent sections.


\section{\titlecap{Vector Space Modeling for \glsentrytext{raga} Recognition}}
\label{sec:phrase_based_feature_extraction}

A number of similarities can be seen between a \gls{raga} rendition and a textual description of a topic. Like an author describes a topic by using different relevant words to the topic, an artist renders a \gls{raga} by using appropriate melodic phrases that suit the context. There are words that are quite specific to a topic, which are analogous to the characteristic melodic phrases of a \gls{raga}. Stop words not specific to any topic or to a document can be seen as analogous to the generic gamaka type melodic patterns (for example, Kampita\footnote{a specific type of an oscillatory melodic movement on a svara~\cite{krishna2012carnatic}.}), which are not specific to a \gls{raga} or to a recording. Words that are specific to a document can be regarded as composition specific melodic patterns. This analogy drives the proposed method, which we denote by $\mathcal{M}_\mathrm{\acrshort{vsm}}$. $\mathcal{M}_\mathrm{\acrshort{vsm}}$ uses automatically discovered melodic patterns and employs concepts of \gls{vsm}. As can be imagined, the overall approach is inspired by the approaches typically used for topic modeling and document classification~\TODO{refs}.

Merits and shortcomings of this method are summarized below:
\begin{itemize}
	\item Human interpretation of the intermediate output, usability of intermediate outputs, exploiting the most characteristic aspect of a raga. Utilize both the tonal and the pitch information. Exploit characteristic movements between the svaras as well as sequence of svaras
	\item No need for analyzing whole melody, if patterns are identified in chunks raga can be identified.
	\item Musically meaningful relatinos between recordings can be established.
\end{itemize}

Cons
\begin{itemize}
	\item Prone to octave errors, not statistical, spurts of error can deprove the performance enormously
	\item Extraction of characteristic melodic patterns is still a challenging process. Limited by that performance
	\item Affected a lot by the improvisational aspects
	\item Computatinoally challenging. 
	\item Does not exploit global melodic characteristics.
\end{itemize}


\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeSeventy]{ch07_ragaRecognition/figures/bd_phasebased_raga_recognition.pdf}
	\end{center}
	\caption{Block diagram of the proposed phrase-based approach to \gls{raga} recognition.}
	\label{fig:bd_phasebased_raga_recognition}
\end{figure}




%In melodic pattern discovery block we follow an unsupervised approach and extract melodic patterns from audio music recordings. For this, we employ our proposed method for mining melodic patterns as described in Section~XXX that scales upto hundreds of hours of music data. These patterns are then clustered together into communities using the method described in Section~XXX. The occurrences of the melodic patterns clustered within communities are used to compute feature vector for each audio recording, which is eventually used for \gls{raga} classification.


\subsection{Feature Extraction}
\label{sec:vsm_feature_extraction}

The block diagram for feature extraction in $\mathcal{M}_\mathrm{\acrshort{vsm}}$ is shown in Figure~\ref{fig:bd_phasebased_raga_recognition}. There are three main processing blocks; melodic pattern discovery, melodic pattern clustering and feature extraction. Below we describe each of these blocks in detail. 

\subsubsection{Melodic Pattern Discovery}
\label{sec:vsm_feature_extraction_pattern_discovery}
In this block we extract melodic patterns from the collection of audio recordings. There are three main processing modules in this block; data-processing, intra-recording pattern discovery and inter-recording pattern search. All these modules are same as described already in Section~XXX. We use the same set of parameter settings as mentioned in their description.

\subsubsection{Melodic Pattern Clustering}
\label{sec:vsm_feature_extraction_pattern_clustering}

In this block we cluster the discovered melodic patterns obtained in the previous step. The objective is to group together all the patterns that are different occurrences of the same melodic phrase. For this, we propose to perform a network analysis in which the clustering is performed using a non-overlapping community detection method. 

We start by building an undirected network $\mathcal{G}$ using the discovered patterns as the nodes of the network. We connect any two nodes only if the distance between them is below a similarity threshold $T_s$. Noticeably, this distance is computed using the same measure as used in the pattern discovery block (Section~XXXX). The weight of the edge, when it exists, is set to 1. Non-connected nodes are removed.

As mentioned before determining a meaningful similarity threshold in an unsupervised manner is a complex task. For estimating an optimal value of $T_s$ we follow the approach as described in Section~\TODO{XXX}. We compare the evolution of the clustering coefficient $C$ of the obtained network $\mathcal{G}$ with a randomized network $\mathcal{G}_r$ over different distance thresholds $T_s$. The randomized network $\mathcal{G}_r$ is obtained by swapping the edges between randomly selected pairs of nodes such that the degree of each node is preserved~\cite{maslov2002specificity}. The optimal threshold $T_{s}^\star$ is taken as the distance that maximizes the difference between the two clustering coefficients.  In Figure~\TODO{XXX}, we show $C(\mathcal{G})$, $C(\mathcal{G}_r)$ and $C(\mathcal{G})-C(\mathcal{G}_r)$ for different values of $T_{s}$, and mark the optimal threshold $T_{s}^\star$.

The next step of our approach (Figure~\ref{fig:bd_phasebased_raga_recognition}) groups similar melodic patterns as done in Section~\TODO{XXX}. To do so, we detect non-overlapping communities in the network of melodic patterns using the method proposed in~\cite{blondel2008fast}. This method is based on optimizing the modularity of the network and is parameter-free from the user's point of view. This method is capable of handling very large networks and has been extensively used in various applications~\cite{fortunato2010community}. We use its implementation available in networkX~\cite{hagberg-2008-exploring}, a Python language package for exploration and analysis of networks and network algorithms. Note that, from now on, the melodic patterns grouped within a community are regarded as the occurrences of a single melodic phrase. Thus, a community essentially represents a melodic phrase or motif.

\subsubsection{TFIDF Feature Extraction}
\label{sec:vsm_feature_extraction_TFID_computation}

As mentioned above, we draw an analogy between r\={a}ga rendition and textual description of a topic. Using this analogy we represent each audio recording using a vector space model. This process is divided into three blocks (Figure~\ref{fig:bd_phasebased_raga_recognition}).

We start by building our vocabulary $P$, which translates to selecting relevant communities (melodic phrases) for characterizing r\={a}gas. For this, we include all the detected communities except the ones that comprise patterns extracted from only a single audio recording. Such communities are analogous to the words that only occur within a document and, hence, are irrelevant for modeling a topic. The size of the obtained vocabulary $P$ corresponding to the optimal threshold mentioned above ($T_{s}^\star=9$) is XXX.

We experiment with three different sets of features $\mathrm{F_1}$, $\mathrm{F_2}$ and $\mathrm{F_3}$, which are similar to the term frequency-inverse document frequency features typically used in text information retrieval. We denote our corpus by $R$  comprising $N = |R|$ number of recordings. A melodic phrase and a recording is denoted by $p$ and $r$ , respectively

\begin{equation}
\mathrm{F_1}(p,r)= 
\begin{cases}
1				,& \text{if}~~f(p,r) > 0\\
0,              & \text{otherwise}
\end{cases}
\end{equation}
where, $f(p,r)$ denotes the raw frequency of occurrence of phrase $p$ in recording $r$. $\mathrm{F_1}$ only considers the presence or absence of a phrase in a recording. In order to investigate if the frequency of occurrence of melodic phrases is relevant for characterizing r\={a}gas, we take $\mathrm{F_2}(p,r) = f(p,r)$. As mentioned, the melodic phrases that occur across different r\={a}gas and in several recordings are futile for r\={a}ga recognition. Therefore, to reduce their effect in the feature vector we employ a weighting scheme, similar to the inverse document frequency ($\mathrm{idf}$) weighting in text retrieval.
\begin{equation}
\mathrm{F_3}(p,r) = f(p,r) \times \mathrm{irf}(p,R)
\end{equation}
\begin{equation}
\mathrm{irf}(p,R) = \log \left( \frac{N}{ |\lbrace r \in R: p \in r \rbrace|} \right)
\end{equation}
where, $|\lbrace r \in R: p \in r \rbrace|$ is the number of recordings where the melodic phrase p is present, that is $f(p,r)\neq 0$ for these recordings. 

The number of melodic patterns obtained from an audio recording tends to be proportional to the length of the recording. In order to reduce the affect of different recording lengths, we investigate two feature vector normalization techniques frequently used in text classification. In addition to using raw features vectors we experiment with $L_1$ and $L_2$ normalization~\cite{leopold2002text}.\TODO{A better reference}. 


\subsection{Classification and Evaluation Methodology}
\label{sec:classification_based_raga_recognition}

The features obtained above are used to train a classifier. In order to assess the relevance of these features for r\={a}ga recognition, we experiment with different algorithms exploiting diverse classification strategies~\cite{Hastie09BOOK}: Multinomial, Gaussian and Bernoulli naive Bayes (NBM, NBG and NBB, respectively), support vector machines with a linear and a radial basis function kernel, and with a stochastic gradient descent learning (SVML, SVMR and SGD, respectively), logistic regression (LR) and random forest (RF). We use the implementation of these classifiers available in scikit-learn toolkit~\cite{scikitlearn}, version 0.15.1. Since in this study, our focus is to extract a musically relevant set of features based on melodic phrases, we use the  default parameter settings for the classifiers available in scikit-learn. 

We use a stratified 12-fold cross-validation methodology for evaluations. The folds are generated such that every fold comprises equal number of feature instances per r\={a}ga. We repeat the entire experiment 20 times, and report the mean classification accuracy as the evaluation measure. In order to assess if the difference in the performance of any two methods is statistically significant, we use the Mann-Whitney U test~\cite{mann1947test} with $p=0.01$. In addition, to compensate for multiple comparisons, we apply the Holm-Bonferroni method~\cite{holm1979simple}. 


\subsection{Results and Discussion}
\label{sec:vsm_eval_results}


\begin{table}
	\tabcolsep=.18cm
	\centering
	\begin{tabular}{c|c|c|c c c c c}
		\hline
		db & Mtd & Ftr & NBM & NBB & LR & SVML & 1NN\tabularnewline
		\hline \hline 
		\multirow{5}{*}{\begin{turn}{90}DB10r\={a}ga\end{turn}} & \multirow{3}{*}{$M$} & $\mathrm{F_1}$ & 90.6 & 74 & 84.1 & 81.2 & -\tabularnewline
		
		&  & $\mathrm{F_2}$ & {\bf 91.7} & 73.8 & 84.8 & 81.2 & -\tabularnewline
		
		&  & $\mathrm{F_3}$ & 90.5 & 74.5 & 84.3 & 80.7 & -\tabularnewline
		\cline{2-8} 
		& \multirow{2}{*}{$S_1$} & $\mathrm{PCD}_{120}$ & - & - & - & - & 82.2\tabularnewline
		& & $\mathrm{PCD}_{full}$ & - & - & - & - & {\bf 89.5}\tabularnewline
		\cline{2-8} 
		& $S_2$ & $\mathrm{PD}_{param}$ & 37.9 & 11.2 & {\bf 70.1} & 65.7 & -\tabularnewline
		\hline \hline
		\multirow{5}{*}{\begin{turn}{90}DB40r\={a}ga\end{turn}} & \multirow{3}{*}{$M$} & $\mathrm{F_1}$ & 69.6 & 61.3 & 55.9 & 54.6 & -\tabularnewline
		
		&  & $\mathrm{F_2}$ & {\bf 69.6} & 61.7 & 55.7 & 54.3 & -\tabularnewline
		
		&  & $\mathrm{F_3}$ & 69.5 & 61.5 & 55.9 & 54.5 & -\tabularnewline
		\cline{2-8} 
		& \multirow{2}{*}{$S_1$} & $\mathrm{PCD}_{120}$ & - & - & - & - & 66.4\tabularnewline
		
		& & $\mathrm{PCD}_{full}$ & - & - & - & - & {\bf 74.1}\tabularnewline
		\cline{2-8} 
		& $S_2$ & $\mathrm{PD}_{param}$ & 20.8 & 2.6 & {\bf 51.4} & 44.2 & -\tabularnewline
		\hline \hline
	\end{tabular}
	
	\caption{Accuracy (in percentage) of different methods (Mtd) for two datasets (db) using different classifiers and features (Ftr).} 
	\label{tab:accuracies_for_variants}
\end{table}



In Table~\ref{tab:accuracies_for_variants}, we present the results of our proposed method $M$ and the two state of the art methods $S_1$ and $S_2$ for the two datasets DB10r\={a}ga and DB40r\={a}ga. The highest accuracy for every method is highlighted in bold for both the datasets. Due to lack of space we present results only for the best performing classifiers. 

We start by analyzing the results of the variants of $M$. From Table~\ref{tab:accuracies_for_variants}, we see that the highest accuracy obtained by $M$ for DB10r\={a}ga is 91.7\%. Compared to DB10r\={a}ga, there is a significant drop in the performance of every variant of $M$ for DB40r\={a}ga. The best performing variant in the latter achieves 69.6\% accuracy. We also see that for both the datasets, the accuracy obtained by $M$ across the feature sets is nearly the same for each classifier, with no statistically significant difference. This suggests that, considering just the presence or the absence of a melodic phrase, irrespective of its frequency of occurrence, is sufficient for r\={a}ga recognition. Interestingly, this finding is consistent with the fact that characteristic melodic phrases are unique to a r\={a}ga and a single occurrence of such phrases is sufficient to identify the r\={a}ga~\cite{krishna2012carnatic}. As seen in Table~\ref{tab:accuracies_for_variants}, the performance of the proposed method is very sensitive to the choice of the classifier. We notice that for both the datasets, the best accuracy is obtained using the NBM classifier, and the difference in its performance compared to any other classifier is statistically significant. Note that, the NBM classifier outperforming other classifiers is also well recognized in the text classification community~\cite{mccallum1998comparison}. We, therefore, only consider the NBM classifier for comparing $M$ with the other methods. It is worth noting that the feature weights assigned by a classifier can be used to identify the relevant melodic phrases for r\={a}ga recognition. These phrases can serve as a dictionary of semantically-meaningful melodic units for many computational tasks in IAM.

In order to validate that the estimated similarity threshold $T_{s}^\star$ (Section~\ref{pattern_clustering}) produces the optimal results, we perform r\={a}ga recognition using different similarity thresholds $T_{s}$. 

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/accuracy_Vs_thresold.pdf}
	\end{center}
	\caption{Accuracy of $M$ and $C(\mathcal{G})-C(\mathcal{G}_r)$ for different similarity thresholds.}
	\label{fig:performance_across_thresholds}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch07_ragaRecognition/figures/cnf_mtx_40raga_with_annotation.pdf}
	\end{center}
	\caption{Confusion matrix for the proposed method. The different shades of grey are mapped to different number of audio recordings.}
	\label{fig:confusion_matrix}
\end{figure}

Before further analyses of our results, we verify here our approach to obtain the optimal similarity threshold $T_{s}^\star$ (Section~\ref{pattern_clustering}). In Figure~\ref{fig:performance_across_thresholds}, we show the accuracy obtained by $M$, and  $C(\mathcal{G})-C(\mathcal{G}_r)$ as a function of similarity threshold $T_{s}$. We see that these curves are highly correlated. Thus, the optimal threshold $T_{s}^\star$, which we defined in Section~\ref{pattern_clustering} as the distance that maximizes the difference $C(\mathcal{G})-C(\mathcal{G}_r)$, also results in the best accuracy for r\={a}ga recognition.

We now analyze the confusion matrix, and understand the type of classification errors made by $M$, (Figure~\ref{fig:confusion_matrix}). We observe that our method achieves near-perfect accuracy for several r\={a}gas including Kaly\={a}\d{n}i, P\={u}rvikaly\={a}\d{n}i, T\={o}\d{d}i and Var\={a}\d{l}i. This is consistent with the fact that these are considered to be phrase-based r\={a}gas, that is, their identity is predominantly derived from melodic phraseology~\cite{krishna2012carnatic}. At the same time, we observe low accuracy for some other phrase-based r\={a}gas such as Madhyam\={a}vati, K\={a}na\d{d}a and \'{S}r\={i}. On investigating further we find that such r\={a}gas are confused often with their allied r\={a}gas\footnote{Allied r\={a}gas have a common set of svaras and similar melodic movement}~\cite{krishna2012carnatic}. Distinguishing between allied r\={a}gas is a challenging task, since it is based on subtle melodic nuances. We also note that, among the other r\={a}gas for which the obtained accuracy is low, several are considered as scale-based r\={a}gas. This is in line with~\cite{krishna2012carnatic}, where the authors remark that the identification of such r\={a}gas is not based on melodic phraseology. Overall, this analysis of the classification errors indicates that our proposed method is more suitable for recognizing phrase-based r\={a}gas compared to scale-based r\={a}gas. 


Finally, we compare $M$ with the state of the art methods $S_1$ and $S_2$. From Table~\ref{tab:accuracies_for_variants}, we see that $M$ outperforms $S_2$ for both the datasets, and the difference is found to be statistically significant. When compared with $S_1$, we see that $M$ performs significantly better than the $\mathrm{PCD}_{120}$ variant of $S_1$ for both the datasets. However, the performance of the $\mathrm{PCD}_{full}$ variant of $S_1$ is comparable to $M$ for DB10r\={a}ga, and, significantly better for DB40r\={a}ga. A comparison of the results of $M$ and $S_1$ for each r\={a}ga reveals that their performance is complementary. $M$ successfully recognizes several r\={a}gas with high accuracy for which $S_1$ performs poorly, and vice-versa. This suggests that the proposed phrase-based method can be combined with the pitch distribution-based methods to achieve a higher r\={a}ga recognition accuracy.



%
%1) What is the motivation behind this method, what can be its advantages
%2) What is the analogy that we have considered to devise the methodology


\section{\titlecap{\glsentrylong{tdms} for \glsentrytext{raga} recognition}}
\label{sec:tdms_raga_recognition}

\TODO{Rephrase intro to something impressive}

The approach described above for \gls{raga} recognition close to how humans identify \glspl{raga} and shows encouraging results. However, several challenges in reliably extracting melodic phrases and shortcomings summarized in Section~\TODO{ref} limit the performance of the system. In this section we propose a novel feature, the \gls{tdms}, that captures several melodic aspects that are useful in characterizing and distinguishing \glspl{raga}. \Glspl{tdms} alleviate some of the shortcomings mentioned above and improves the accuracy of \gls{raga} recognition by large margins. \Gls{tdms} is inspired by the concept of delay coordinates~\TODO{ref}. The main strengths of \gls{tdms} are:

\begin{itemize}
	\item It is a compact representation that describes both the tonal and the temporal characteristics of a melody
	\item It simultaneously captures the melodic characteristics at different time-scales, the overall usage of the pitch-classes in the entire recording, and the short-time temporal relation between individual pitches.
	\item It is robust to pitch octave errors.
	\item It does not require the transcription of the melody nor a discrete representation of it.
	\item It is easy to implement, fast to compute, and has a musically-meaningful interpretation.
	\item As it will be shown, it obtains unprecedented accuracies in the raga recognition task, outperforming the state-of-the-art by a large margin, without the use of any elaborated classification schema.
\end{itemize}


\subsection{\titlecap{\glsentrylong{tdms}}}
\label{sec:tdms_feature_extraction}

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeSixty]{ch07_ragaRecognition/figures/tdms_computation.pdf}
	\end{center}
	\caption{Block diagram for \gls{tdms} compuation}
	\label{fig:bd_tdms_computation}
\end{figure}

The computation of a \gls{tdms} has three steps as shown in Figure~\ref{fig:bd_tdms_computation}: pre-processing, surface generation, and post-processing. In pre-processing, we obtain a representation of the melody of an audio recording, which is normalized by the tonic or base frequency of the music piece. In surface generation, we compute a two dimensional surface based on the concept of delay coordinates. Finally, in post-processing, we apply power compression and Gaussian smoothing to the computed surface. We subsequently detail these steps.

\subsubsection{Pre-processing}
\label{sec:tdms_preprocessing}

In this step we process the audio collection to obtain representation of the melodies that is suitable for the task of \gls{raga} recognition. For the computation of the \gls{tdms} we represent melody of an audio excerpt by the pitch of the predominant melodic source. For predominant pitch estimation we follow the method described in Section~\TODO{XXX}. We use exactly the same set of parameter values for pitch estimation. We subsequently post-process the estimated pitch to smoothen it and remove spurious pitch jumps. For post-processing we follow the procedure described in Section~\TODO{XXX} and use the same set of parameter values.

The base frequency chosen for a melody in IAM is the tonic pitch of the lead artist~\cite{Gulati2014Tonic}, to which all other accompanying instruments are tuned. Therefore, a musically meaningful feature for \gls{raga} recognition should be independent of the tonic of the lead artist. For this, we normalize the predominant melody of every recording by considering its tonic pitch as the reference frequency during the Hertz-to-cent-scale conversion. For automatically identifying tonic values and subsequently converting pitch from Hertz to cent scale we follow the procedure detailed in Section~\TODO{XXX}.


\subsubsection{Surface Generation}
\label{sec:tdms_surface_generation}

The next step is to construct a two-dimensional surface based on the concept of delay coordinates (also termed phase space embedding)~\cite{takens1981detecting, Kantz04BOOK}. In fact, such two-dimensional surface can be seen as a discretized histogram of the elements in a two-dimensional Poicar\'e map~\cite{Kantz04BOOK}. For a given recording, we generate a surface $\check{\mathbf{S}}$ of size $\eta\times\eta$ recursively, by computing
\begin{equation*}
%\label{eq:surface_computation1}	
\check{s}_{ij} = \sum_{t=\tau}^{N-1} I\left(B\left(c_t\right),i\right)~ I\left(B\left(c_{t-\tau}\right),j\right) %\left(1-I\left(f_tf_{t-\tau},0\right)\right)
\end{equation*}
%\vspace{-2em}
for $0 \leq i,j < \eta$, where $I$ is an indicator function such that $I(x,y)=1$ iff $x=y$, $I(x,y)=0$ otherwise, $B$ is an octave-wrapping integer binning operator defined by
\begin{equation}	
\label{eq:binning_function}	
B(x) = \left\lfloor ~\left(\frac{\eta x}{1200}\right) \bmod \eta ~\right\rfloor,
\end{equation}
and $\tau$ is a time delay index (in frames) that is left as a parameter. Note that, as mentioned, the frames where a predominant pitch could not be obtained are excluded from any calculation. For the size of $\check{\mathbf{S}}$ we use $\eta = 120$. This value  corresponds to 10 cents per bin, an optimal pitch resolution reported in~\cite{chordia2013joint}. %The study showed that r\={a}ga recognition using PCDs with a bin width of 10~cents outperforms the PCDs with a bin width of 100~cents, and obtains comparable results to PCDs with a bin width of 5~cents.

An example of the generated surface $\check{\mathbf{S}}$ for a music piece\footnote{http://musicbrainz.org/recording/e59642ca-72bc-466b-bf4b-d82bfbc7b4af} in r\={a}ga Yaman is shown in Figure~\ref{fig:phase_space_surface}\,(a). We see that the prominent peaks in the surface correspond to the svaras of r\={a}ga Yaman. We notice that these peaks are steep and that the dynamic range of the surface is high. This can be attributed to the nature of the melodies in these music traditions, particularly in Hindustani music, where the melodies often contain long held svaras. In addition, the dynamic range is high because the pitches in the stable svara regions are within a small range around the svara frequency compared to the pitches in the transitory melodic regions. Because of this, the frequency values in the stable regions are mapped to a smaller set of bins, making the prominent peaks more steep.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/PSfeature_e59642ca-72bc-466b-bf4b-d82bfbc7b4af.pdf}
	\end{center}\vspace{-1.5em}
	\caption{Generated surface for a music piece before (a) and after (b) applying post-processing ($\check{\mathbf{S}}$ and $\hat{\mathbf{S}}$, respectively). For ease of visualization, both matrices are normalized here between 0 and 1.}
	\label{fig:phase_space_surface}
	\vspace{-0.5em}
\end{figure}

\subsubsection{Post-processing}
\label{sec:tdms_post_processing}

In order to accentuate the values corresponding to the transitory regions in the melody and reduce the dynamic range of the surface, we apply an element-wise power compression %$\overline{\mathbf{S}}= \mathbf{S}^\alpha$ %described in \eqnref{eq:power_compression}. 
\begin{equation*}
%\label{eq:power_compression}	
\overline{\mathbf{S}}= \check{\mathbf{S}}^\alpha ,
\end{equation*}
where $\alpha$ is an exponent that is left as a parameter. Once a more compact (in terms of the dynamic range) surface is obtained, we apply Gaussian smoothing. With that, we attempt to attenuate the subtle differences in $\overline{\mathbf{S}}$ corresponding to the different melodies within the same r\={a}ga, while retaining the attributes that characterize that r\={a}ga. 

We perform Gaussian smoothing by circularly convolving $\overline{\mathbf{S}}$ with a two-dimensional Gaussian kernel. We choose a circular convolution because of the cyclic (or octave-folded) nature of the \gls{tdms} (Equation~\ref{eq:binning_function}), which mimics the cyclic nature of pitch classes. The standard deviation of this kernel is $\sigma$ bins (samples). The length of the kernel is truncated to $8\sigma + 1$ bins in each dimension, after which the values are negligible (below $0.01\%$ of the kernel's maximum amplitude). We experiment with different values of $\sigma$, and also with a method variant excluding the Gaussian smoothing (loosely denoted by $\sigma = -1$), so that we can quantify its influence on the accuracy of the system. 
%
%After convolution we retain the same dimensionality of the surface as before by discarding $4\sigma$ number of samples from the start and the end of each dimension (i.e.~the border). 

Once we have the smoothed surface $\hat{\mathbf{S}}$, there is only one step remaining to obtain the final \gls{tdms}. Since the overall duration of the recordings and of the voiced regions within them is different, the computed surface $\hat{\mathbf{S}}$ needs to be normalized. To do so, we divide $\hat{\mathbf{S}}$ by its $L_1$ matrix norm:
\begin{equation*}
\mathbf{S} = \hat{\mathbf{S}}/\norm{\hat{\mathbf{S}}}_{1} .
\end{equation*}
This also yields values of $\mathbf{S}$, the final TDMS, that are interpretable in terms of discrete probabilities.

The result after post-processing the surface of Figure~\ref{fig:phase_space_surface}\,(a) with power compression and Gaussian smoothing is shown in Figure~\ref{fig:phase_space_surface}\,(b). We see that the values corresponding to the non-diagonal elements are accentuated. A visual inspection of Figure~\ref{fig:phase_space_surface}\,(b) provides several musical insights to the melodic aspects of the recording. For instance, the high salience indices along the diagonal, $(0,0)$, $(20,20)$, $(40,40)$, $(60,60)$, $(70,70)$, $(90,90)$, and $(110,110)$, correspond to the 7~svaras used in r\={a}ga Yaman. Within which, the highest salience at indices (110,110) correspond to the Ni svara, which is the {\it Vadi} svara, i.e., musically the most salient svara of the r\={a}ga, in this case r\={a}ga Yaman~\cite{rao1999raga}. The asymmetry in the matrix with respect to the diagonal indicates the asymmetric nature of the ascending and descending svara pattern of the r\={a}ga (compare, for example, the salience at indices $(70, 90)$ to indices $(90, 70)$, with the former being more salient than the latter). The similarity of the matrix between indices $(20,20)$ and $(70,70)$ with respect to the matrix between indices $(70,70)$ and $(120,120)$ delineates the tetra-chord structure of the r\={a}ga. Finally, it should be noted that an interesting property of TDMSs is that the mean of the sum across its row and columns yields a PCD representation (see Section \TODO{XXX}).

\subsection{Classification and Distance Measures}
\label{sec:tdms_classification_evaluation}

% Our second method  $\mathcal{M}_\mathrm{\acrshort{tdms}}$ uses a novel melodic representation, \gls{tdms}, for compactly capturing both the tonal and the temporal characteristics of \glspl{raga}. Evaluations done using representative and scalable data sets for both Hindustani and Carnatic music demonstrate that $\mathcal{M}_\mathrm{\acrshort{tdms}}$ outperforms state-of-the-art methods by a large margin. 
% 
%1) Motivation behind this method
%2) Desired qualities of this method


In order to demonstrate the ability of the \gls{tdms} s in capturing r\={a}ga characteristics, we consider the task of classifying audio recordings according to their r\={a}ga label. To perform classification, we choose a $k$-nearest neighbor (kNN) classifier~\cite{Mitchell97BOOK}. The reasons for our choice are manifold. Firstly, the kNN classifier is well understood, with well studied relations to other classifiers in terms of both performance and architecture. Secondly, it is fast, with practically no training and with known techniques to speed up testing or retrieval. Thirdly, it has only one parameter, $k$, which we can just blindly set to a relatively small value or can easily optimize in the training phase. Finally, it is a classifier that is simple to implement and whose results are both interpretable and easily reproducible. 

The performance of a kNN classifier highly depends on the distance measure used to retrieve the $k$ neighbors. We consider three different measures to compute the distance between two recordings $n$ and $m$ with \gls{tdms}\ features $\mathbf{S}^{(n)}$ and $\mathbf{S}^{(m)}$, respectively. We first consider the Frobenius norm of the difference between $\mathbf{S}^{(n)}$ and $\mathbf{S}^{(m)}$,
\begin{equation*}
D_{\mathrm{F}}^{(n,m)} = \norm{\mathbf{S}_{n} - \mathbf{S}_{m}}_2 .
\end{equation*}
Next, we consider the symmetric Kullback-Leibler divergence
\begin{equation*}
D_{\mathrm{KL}}^{(n,m)} = D_{\mathrm{KL}}\left(\mathbf{S}^{(n)},\mathbf{S}^{(m)}\right) + D_{\mathrm{KL}}\left(\mathbf{S}^{(m)},\mathbf{S}^{(n)}\right) ,
\end{equation*}
with
\begin{equation*}
D_{\mathrm{KL}}\left(\mathbf{X},\mathbf{Y}\right) = \sum{\mathbf{X} \log\left(\frac{\mathbf{X}}{\mathbf{Y}} \right)} ,
\end{equation*}
where we perform element-wise operations and sum over all the elements of the resultant matrix. Finally, we consider the Bhattacharyya distance, which is reported to outperform other distance measures with a PCD-based feature for the same task in~\cite{chordia2013joint},
\begin{equation*}
D_{\mathrm{B}}^{(n,m)} = -\log\left( \sum{ \sqrt{ \mathbf{S}^{(n)} \cdot \mathbf{S}^{(m)} } } \right) .
\end{equation*}
We again perform element-wise operations and sum over all the elements of the resultant matrix. Variants of our proposed method that use $D_{\mathrm{F}}$, $D_{\mathrm{KL}}$ and $D_{\mathrm{B}}$ are denoted by $\mathcal{M}_{\mathrm{F}}$, $\mathcal{M}_{\mathrm{KL}}$, and $\mathcal{M}_{\mathrm{B}}$, respectively.



\subsection{Evaluation, Results and Discussion}
\label{sec:tdms_eval_results}

To evaluate the performance of the considered methods we use the raw overall accuracy~\cite{Mitchell97BOOK}. Since both CMD and HMD are balanced in the number of instances per class, we do not need to correct such raw accuracies to counteract for possible biases towards the majority class. We perform a leave-one-out cross validation~\cite{Mitchell97BOOK}, in which one recording from the evaluation data set forms the testing set and the remaining ones become the training set. To assess if the difference in the performance between any two methods is statistically significant, we use McNemar's test~\cite{mcnemar1947note} with $p < 0.01$. To compensate for multiple comparisons, we apply the Holm-Bonferroni method~\cite{holm1979simple}. Besides accuracy, and for a more detailed error analysis, we also compute the confusion matrix over the predicted classes.

In the case of $\mathcal{M}$, a test recording is assigned the majority class of its $k$-nearest neighbors obtained from the training set and, in case of a tie, one of the majority classes is selected randomly. Because we conjecture that none of the parameters we consider is critical to obtain a good performance, we initially make an educated guess and intuitively set our parameters to a specific combination. We later study the influence of every parameter starting from that combination. We initially use $\tau=0.3$\,s, $\alpha=0.75$, $\sigma=2$, and $k=1$, and later consider $\tau\in\lbrace 0.2, 0.3, 0.5, 1, 1.5\rbrace$\,s, $\alpha\in\lbrace 0.1, 0.25, 0.5, 0.75, 1 \rbrace$, $\sigma\in\lbrace -1, 1, 2, 3\rbrace$, and $k\in\lbrace 1,3,5\rbrace$ (recall that $\sigma=-1$ corresponds to no smoothing; Section~\TODO{XXX}.


In Table~\ref{tab:main_results}, we show the results for all the variants of the proposed method $\mathcal{M}_{\mathrm{F}}$, $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$, and the two state-of-the-art methods $\mathcal{E}_{\mathrm{PCD}}$ and $\mathcal{E}_{\mathrm{VSM}}$, using HMD and CMD data sets. We see that the highest accuracy obtained on HMD is 97.7\% by $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$. This accuracy is considerably higher than the 91.7\% obtained by $\mathcal{E}_{\mathrm{PCD}}$, and the difference is found to be statistically significant. We also see that $\mathcal{E}_{\mathrm{PCD}}$ performs significantly better than $\mathcal{E}_{\mathrm{VSM}}$. Regarding the proposed variants, we see that, in HMD, $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$ perform better than $\mathcal{M}_{\mathrm{F}}$, with a statistically significant difference. 

\begin{table}[t] 
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{ c | c c c | c c}
			\hline\hline
			Data set   	& 	$\mathcal{M}_{\mathrm{F}}$ 	&	$\mathcal{M}_{\mathrm{KL}}$		&	$\mathcal{M}_{\mathrm{B}}$	&	$\mathcal{E}_{\mathrm{PCD}}$		&	$\mathcal{E}_{\mathrm{VSM}}$\\	
			\hline
			HMD   	& 	91.3 	&	{\bf 97.7}		&	{\bf 97.7} 	&	91.7		&	83.0\\	
			
			CMD   	& 	81.5	&	{\bf 86.7}		&	{\bf 86.7}	&	73.1		&	68.1\\	
			\hline\hline
		\end{tabular}
	}
	\caption{Accuracy (\%) of the three proposed variants, $\mathcal{M}_{\mathrm{F}}$, $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{BC}}$, and the two existing state-of-the-art methods $\mathcal{E}_{\mathrm{PCD}}$ and $\mathcal{E}_{\mathrm{VSM}}$ (see text). The random baseline for this task is 3.3\% for HMD and 2.5\% for CMD. }
	\label{tab:main_results}
\end{table}

In Table~\ref{tab:main_results}, we see that the trend in the performance for  CMD across different methods is similar to that for  HMD. The variants $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$ achieve the highest accuracy of 86.7\%, followed by $\mathcal{E}_{\mathrm{PCD}}$ with 73.1\%. The difference between $\mathcal{M}_{\mathrm{KL}}$ ($\mathcal{M}_{\mathrm{B}}$) and $\mathcal{E}_{\mathrm{PCD}}$ is found to be statistically significant.  For CMD, also  $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$ perform better than $\mathcal{M}_{\mathrm{F}}$, with a statistically significant difference. 

In general, we notice that, for every method, the accuracy is higher on HMD compared to CMD. This, as expected, can be largely attributed to the difference in the number of classes in  HMD (30 r\={a}gas) and  CMD (40 r\={a}gas). A higher number of classes makes the task of r\={a}ga recognition more challenging for  CMD, compared to  HMD. In addition to that, another factor that can cause this difference could be the length of the audio recordings, which for  HMD are significantly longer than the ones in CMD.

As mentioned earlier, the system parameters corresponding to the results in Table~\ref{tab:main_results} were set intuitively, without any parameter tuning. Since \gls{tdms} s are used here for the first time, we want to carefully analyze the influence that each of the parameters has on the final r\={a}ga recognition accuracy, and ultimately perform a quantitative assessment of their importance. In Figure~\ref{fig:accuracy_vs_parameter_values}, we show the accuracy of $\mathcal{M}_{\mathrm{KL}}$ for different values of these parameters. In each case, only one parameter is varied and the rest are set to the initial values mentioned above. 

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=\figSizeEighty]{ch07_ragaRecognition/figures/accuracy_vs_parameters_long2.pdf}
	\end{center}\vspace{-1em}
	\caption{Accuracy of $\mathcal{M}_{\mathrm{KL}}$ as a function of parameter values. State-of-the-art approaches $\mathcal{E}$ and random baselines $\mathcal{B}$ are also reported for comparison.} 
	\label{fig:accuracy_vs_parameter_values}
\end{figure}

In Figure~\ref{fig:accuracy_vs_parameter_values}\,(a), we observe that the performance of the method is quite invariant to the choice of $\tau$, except for the extreme delay values of 1 and 1.5\,s for CMD. This can be attributed to the melodic characteristics of Carnatic music, which presents a higher degree of oscillatory melody movements and shorter stationary svara regions, as compared to Hindustani music. In Figure~\ref{fig:accuracy_vs_parameter_values}\,(b), we see that compression with $\alpha < 1$ slightly improves the performance of the method for both data sets. However, the performance degrades for $\alpha < 0.75$ for  CMD and $\alpha < 0.25$ for  HMD. This again appears to be correlated with the long steady nature of the svaras in Hindustani music melodies. Because the dynamic range of $\check{\mathbf{S}}$ is high, \gls{tdms}\ features require a lower value for the compression factor $\alpha$ to accentuate the surface values corresponding to the transitory regions in the melodies of Hindustani music. In Figure~\ref{fig:accuracy_vs_parameter_values}\,(c), we observe that Gaussian smoothing significantly improves the performance of the method, and that such performance is invariant across the chosen values of $\sigma$. Finally, in Figure~\ref{fig:accuracy_vs_parameter_values}\,(d), we notice that the accuracy decreases with increasing $k$. This is also expected due to the relatively small number of samples per class in our data sets~\cite{Mitchell97BOOK}. %The best accuracy obtained is for $k=1$, which is also reported in a previous study that uses PCD-based feature for the same task~\cite{chordia2013joint}. 
Overall, the method appears to be invariant to different parameter values to a large extent, which implies that it is easier to extend and tune it to other data sets.

\begin{figure}
	\begin{center}
		\includegraphics[width=\figSizeHundred]{ch07_ragaRecognition/figures/carnaticPhaseSapce_config_1078.pdf}
	\end{center}\vspace{-1em}
	\caption{Confusion matrix of the predicted r\={a}ga labels obtained by $\mathcal{M}_{\mathrm{KL}}$ on CMD. Shades of grey are mapped to the number of audio recordings.} 
	\label{fig:confusion_mtx_carnatic}\vspace{-1em}
\end{figure}

From the results reported in Figure~\ref{fig:accuracy_vs_parameter_values}, we see that there exist a number of parameter combinations that could potentially yield a better accuracy than the one reported in Table~\ref{tab:main_results}. For instance, using  $\tau=0.3$\,s, $\alpha=0.5$, $\sigma=2$, and $k=1$, we are able to reach 97.0\% for $\mathcal{M}_{\mathrm{F}}$ and 98.0\% for both $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$ on HMD. These accuracies are ad-hoc, optimizing the parameters on the testing set. However, and doing things more properly, we could learn the optimal parameters in training, through a standard grid search, cross-validated procedure over the training set~\cite{Mitchell97BOOK}. As our primary goal here is not to obtain the best possible results, but to show the usefulness and superiority of \gls{tdms} s, we do not perform such an exhaustive parameter tuning and leave it for future research.

%\begin{table} 
%	\centering
%	\resizebox{\linewidth}{!}{
%	\begin{tabular}{ c | c c c | c c}
%		\hline\hline
%		Data set   	& 	$\mathcal{M}_{\mathrm{F}}$ 	&	$\mathcal{M}_{\mathrm{KL}}$		&	$\mathcal{M}_{\mathrm{B}}$	&	$\mathcal{E}_{\mathrm{PCD}}$		&	$\mathcal{E}_{\mathrm{VSM}}$\\	
%		\hline
%		HMD   	& 	97.0 	&	{\bf 98.0}		&	{\bf 98.0} 	&	91.7		&	83.0\\
%		CMD   	& 	86.3	&	{\bf 86.7}		&	{\bf 86.7}	&	73.1		&	69.1\\
%		\hline\hline
%	\end{tabular}
%	}
%	\caption{Best in-sample accuracies (\%) of the three variants of the proposed method $\mathcal{M}_{\mathrm{F}}$, $\mathcal{M}_{\mathrm{KL}}$ and $\mathcal{M}_{\mathrm{B}}$, and its comparison with the considered existing state-of-the-art methods $\mathcal{E}_{\mathrm{PCD}}$ and $\mathcal{E}_{\mathrm{VSM}}$.}
%	\label{tab:main_results_overfit}
%\end{table}


To conclude, we proceed to analyze the errors made by the best performing variant $\mathcal{M}_{\mathrm{KL}}$. For  CMD, we show the confusion matrix of the predicted r\={a}ga labels in Figure~\ref{fig:confusion_mtx_carnatic}. In general, we see that the confusions have a musical explanation. The majority of them are between the r\={a}gas in the sets $\lbrace$Bhairavi, Mukh\={a}ri$\rbrace$, $\lbrace$Harik\={a}mbh\={o}ji, K\={a}mbh\={o}ji$\rbrace$, $\lbrace$Madhyamvat\={i}, A\d{t}\={a}na, \'Sr\={i}$\rbrace$, and $\lbrace$K\={a}pi, \={A}nandabhairavi$\rbrace$. R\={a}gas within each of these sets are allied r\={a}gas~\cite{Viswanathan2004}, i.e., they share a common set of svaras and similar phrases. For HMD, there are only 7 incorrectly classified recordings (confusion matrix omitted for space reasons). R\={a}ga Alhaiy\={a} bil\={a}wal and r\={a}ga D\={e}\'{s} is confused with r\={a}ga Gau\d{d} Malh\={a}r, which is musically explicable as these r\={a}gas share exactly the same set of svaras. R\={a}ga R\={a}g\={e}\'{s}hr\={i} is confused with B\={a}g\={e}\'{s}hr\={i}, which differ in only one svara. In all these cases, the r\={a}gas which are confused also have similar melodic phrases. For two specific cases of confusions, that of r\={a}ga  Kham\={a}j with B\={a}g\={e}\'{s}hr\={i}, and r\={a}ga Darb\={a}r\={i} with Bh\={u}p, we find that the error lies in the estimation of the tonic pitch.


\section{Comparison with state of the art}
\label{sec:summary_raga_recognition}

\section{Summary}
\label{sec:summary_raga_recognition}




